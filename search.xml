<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spark基础]]></title>
    <url>%2Fblog%2F2019%2F03%2F09%2Fspark%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[spark基础1、目标 1、掌握spark相关概念 2、掌握搭建一个spark集群 3、掌握开发简单的spark应用程序 2、spark概述 2.1 spark是什么 Apache Spark™ is a unified analytics engine for large-scale data processing. apache的spark是一个针对于大规模数据处理的统一分析引擎 spark是基于内存计算的分布式分析引擎，计算速度非常快，这里仅仅只涉及到数据的计算，并没有涉及到数据的存储，后期再使用spark的时候，就需要对接一些外部的数据源（比如HDFS） 2.2 为什么要学习spark1由于spark的处理速度比mapreduce快很多，很受企业青睐，所以我今天就重点学习它 2.3 spark框架的四大特性 1、速度快 spark比mapreduce处理任务在内存中快100倍，在磁盘中快10倍 spark比mapreduce速度快的2个主要原因 12345（1） mapreduce任务每一次job的输出结果只能够保存在磁盘中，后续有其他的job需要依赖于前面job的输出结果，这个时候只能够进行大量的磁盘io操作，获取得到。spark任务每一次的job输出结果可以直接保存在内存中，后续有其他的job需要依赖于前面job的输出结果，这个时候就可以直接从内存中获取得到。大大减少磁盘io操作，最后整体上提升性能。 hivesql: select name,age,sex from (select * from user where age &gt;30) (2) mapreduce任务它是以进程的方式运行在yarn中，比如一个mapreduce任务中，有100个MapTask,这个时候就需要开启100个进程去运行着100个task；spark任务的它是以线程的方式运行在进程中，比如一个spark任务中还是有100个task，后期再运行的时候，可以极端一点，开启一个进程，在这一个进程中运行100个线程即可。 开启一个进程跟开启一个线程的代价肯定是不一样，开启一个进程需要的时间远远高于开启一个线程需要的时间。 2、易用性 可以快速开发一个spark应用程序，通过java、scala、python、R、SQL等不同的语言进行代码开发 3、通用性 spark框架不在是一个简单的框架，它发展成一个spark的生态系统，它是包括了很多不同的子项目 Sparksql sparkStreaming Mlib Graphx 4、兼容性 spark程序就是一个计算任务，哪里可以给当前这个任务提供计算资源，我们就可以把spark程序提交到哪里去运行 standAlone 它是spark集群自带的模式，整个任务的资源分配由spark集群中的老大Master节点负责 yarn 可以把spark程序提交到yarn中去运行，整个任务的资源分配由yarn中的老大ResouceManager mesos 它也是一个类似于yarn的开源的资源调度框架 3、spark集群安装部署 1、下载对应的安装包 http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.1.3/spark-2.1.3-bin-hadoop2.7.tgz spark-2.1.3-bin-hadoop2.7.tgz 2、规划安装目录 /export/servers 3、上传安装包到服务器中 4、解压安装包到指定的规划目录 tar -zxvf spark-2.1.3-bin-hadoop2.7.tgz -C /export/servers 5、重命名解压目录 mv spark-2.1.3-bin-hadoop2.7 spark 6、修改配置文件 进入到spark的安装目录有一个conf文件夹 vim spark-env.sh ( mv spark-env.sh.template spark-env.sh) 123456#配置java环境变量export JAVA_HOME=/export/servers/jdk#指定master的地址export SPARK_MASTER_HOST=node1#指定master的端口export SPARK_MASTER_PORT=7077 vim slaves (mv slaves.template slaves) 123#指定哪些节点是workernode2node3 7、配置spark环境变量 vim /etc/profile 12export SPARK_HOME=/export/servers/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 8、分发spark目录和环境变量 12345scp -r spark node2:/export/serversscp -r spark node3:/export/serversscp /etc/profile node2:/etcscp /etc/profile node3:/etc 9、让所有spark节点的环境变量生效 可以在所有节点执行 source /etc/profile 4、spark集群的启动和停止 1、启动spark集群 在主节点执行脚本 sbin/start-all.sh 首先在主节点启动了一个Master进程，它是整个spark集群的老大，负责任务的资源分配 通过slaves文件执行的worker节点来分别启动worker进程 2、停止spark集群 在主节点执行脚本 sbin/stop-all.sh 5、spark集群的web管理界面 1、启动好spark集群之后 可以访问地址 master主机名:8080 可以看到spark集群的所有信息 spark集群的总的资源信息 spark集群已经使用的资源信息 spark集群还剩的资源信息 spark集群中每一个worker的相关信息 spark集群中正在运行的任务信息 spark集群中已经完成的任务信息 6、基于zk构建高可用的spark集群 1、事先搭建一个zk集群 2、修改配置 vim spark-env.sh 12345#把之前手动指定哪一个节点是活着的master这个参数注释掉#export SPARK_MASTER_HOST=node1#引入zk，构建高可用的spark集群export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark" 3、分发文件到其他机器 12scp spark-env.sh node2:/export/servers/spark/confscp spark-env.sh node3:/export/servers/spark/conf 4、先启动zk 5、启动spark集群 1、可以在任意一台机器来启动一个脚本（前提条件：实现任意2台机器之间的ssh免密登录） sbin/start-all.sh 2、它会在当前机器启动一个Master进程（活着的master） 3、整个spark集群中的worker进程由slaves文件决定 4、在其他机器单独启动master sbin/start-master.sh master恢复逻辑 12345678910首先基于引入了zk集群，这个时候整个spark集群中有很多个master，其中一个master被选举成活着的master，它提供服务，后期给任务分配资源。还有其他多个master被选举成备用的master（standBy）,它不提供服务。当前活着的master挂掉了，首先zk会感知到，接下来在所有处于standBy中的master进行选举，最后生成一个活着的master，后期需要从zk集群中读取保存了spark集群的元数据节点spark。最后恢复到上一次master的状态，整个恢复需要一定的时间，一般就是1-2分钟。整个spark集群master正处于恢复阶段，对任务有没有影响？（1）对于正在运行的任务有没有影响，没有影响。 由于任务正在运行，就说明它已经获取得到了资源，既然有资源了，就跟master没有关系，任务可以继续运行。这里就是没有任何影响 （2）对于正准备提交的任务有没有影响，有影响。 由于没有这样一个活着的master去分配资源，也就说任务获取不到资源，既然没有资源，任务也就无法运行，必须等到活着的master出现之后，才可以申请到资源。 7、spark集群架构 1、Driver端 它会运行客户端的main方法和构建SparkContext对象，SparkContext对象是所有spark程序的执行入口 2、Application 它就是一个spark的应用程序，包括了Driver的代码和任务运行的时候需要的资源信息 3、ClusterManager 它是给计算任务提供计算资源的外部服务 standAlone 它是spark集群自带的模式，任务的资源分配由老大Master节点负责 yarn 可以把spark程序提交到yarn中去运行，任务的资源分配由yarn中老大ResourceManager负责 mesos 它也是一个类似于yarn资源调度平台 4、Master 它是整个spark集群的老大，负责资源的分配 5、Worker 它是整个spark集群的小弟，负责任务的计算节点 6、Executor 它是一个进程，在这里表示计算的资源 它会在worker节点启动executor进程 7、task 它就是一个线程 它是以task线程的方式运行在worker节点对应的executor进程中 8、初识spark程序 1、普通模式提交任务（就是我们已经知道了集群中活着的master地址） 1234567bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node-1:7077 \--executor-memory 1G \--total-executor-cores 2 \examples/jars/spark-examples_2.11-2.1.3.jar \100 2、高可用模式下提交任务（就是集群中有很多个master，这个时候我也不知道哪一个master是活着的master） code here123456789101112bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node-1:7077,node-2:7077,node-3:7077 \--executor-memory 1G \--total-executor-cores 2 \examples/jars/spark-examples_2.11-2.1.3.jar \100后期再实际环境中，有很多个master，任务的提交需要找到活着的master去申请资源，这个时候我们不知道哪一个master是活着的master，这里我们就可以把所有的master都罗列出来：--master spark://node1:7077,node2:7077,node3:7077后期程序在运行的时候，它会通过轮训的机制最后找到活着的master，然后向活着的master申请计算资源。 9、spark-shell使用9.1 通过spark-shell –master local[N] 读取本地数据文件实现单词统计 –master local[N] local表示本地运行，跟spark集群没有任何关系 N表示一个正整数，在这里local[N] 表示本地采用N个线程去跑任务 spark-shell –master local[2] 代码 123sc.textFile("file:///root/words.txt").flatMap(x=&gt;x.split(" ")).map(x=&gt;(x,1)).reduceByKey((x,y)=&gt;x+y).collectsc.textFile("file:///root/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect 9.2 通过spark-shell –master local[N] 读取HDFS数据文件实现单词统计 spark整合HDFS vim spark-env.sh 12#spark整合HDFSexport HADOOP_CONF_DIR=/export/servers/hadoop/etc/hadoop spark-shell –master local[2] 代码 123sc.textFile("hdfs://node1:9000/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collectxsc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect 9.3 通过spark-shell 指定具体活着的master实现HDFS上文件单词统计，把结果数据保存到hdfs上 spark-shell –master spark://node2:7077 –executor-memory 1g –total-executor-cores 4 代码 1sc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("/out") 10、通过IDEA开发spark的程序10.1 利用scala语言开发spark的wordcount程序（本地运行） 1、引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.3&lt;/version&gt;&lt;/dependency&gt; 2、代码开发 12345678910111213141516171819202122232425262728293031323334353637383940package cn.itcast.sparkimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;//todo:利用scala语言开发spark的wordcount程序（本地运行）object WordCount &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 设置application名称和master地址 local[2]表示本地采用2个线程跑任务 val sparkConf: SparkConf = new SparkConf().setAppName("WordCount").setMaster("local[2]") //2、创建SparkContext对象 它是所有spark程序的执行入口，它内部会构建DAGScheduler和TaskScheduler val sc = new SparkContext(sparkConf) //设置日志输出级别 sc.setLogLevel("warn") //3、读取数据文件 val data: RDD[String] = sc.textFile("E:\\words.txt") //4、切分每一行，获取所有的单词 val words: RDD[String] = data.flatMap(x=&gt;x.split(" ")) //5、把每个单词计为1 val wordAndOne: RDD[(String, Int)] = words.map(x=&gt;(x,1)) //6、相同单词出现的1累加 val result: RDD[(String, Int)] = wordAndOne.reduceByKey((x:Int,y:Int)=&gt;x+y) //按照单词出现的次数降序排列 默认第二个参数是true表示升序，改为false为降序 val sortRDD: RDD[(String, Int)] = result.sortBy(x=&gt;x._2,false) //7、收集打印 val finalResult: Array[(String, Int)] = sortRDD.collect() finalResult.foreach(println) //8、关闭 sc.stop() &#125;&#125; 10.2 利用scala语言开发spark的wordcount程序（集群运行） 1、代码开发 1234567891011121314151617181920212223242526272829303132333435363738package cn.itcast.sparkimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDD//todo:利用scala语言开发spark的wordcount程序（集群运行）object WordCount_Online &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 设置application名称 val sparkConf: SparkConf = new SparkConf().setAppName("WordCount_Online") //2、创建SparkContext对象 它是所有spark程序的执行入口，它内部会构建DAGScheduler和TaskScheduler val sc = new SparkContext(sparkConf) //设置日志输出级别 sc.setLogLevel("warn") //3、读取数据文件 val data: RDD[String] = sc.textFile(args(0)) //4、切分每一行，获取所有的单词 val words: RDD[String] = data.flatMap(x=&gt;x.split(" ")) //5、把每个单词计为1 val wordAndOne: RDD[(String, Int)] = words.map(x=&gt;(x,1)) //6、相同单词出现的1累加 val result: RDD[(String, Int)] = wordAndOne.reduceByKey((x:Int,y:Int)=&gt;x+y) //7、把结果数据保存到hdfs上 result.saveAsTextFile(args(1)) //8、关闭 sc.stop() &#125;&#125; 2、把程序打成jar包提交到spark集群中运行 1234567spark-submit \--master spark://node-1:7077 \--class cn.itcast.spark.WordCount_Online \ #jar包的main方法路径--executor-memory 1G \--total-executor-cores 2 \original-spark_demo-1.0-SNAPSHOT.jar \ #需要在当前路径上有才能这么些，否则需要写全路径/words.txt /out_spark 10.3 利用java语言开发spark的wordcount程序（本地运行） 1、代码开发 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package cn.itcast.spark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;import java.util.List;//todo:利用java语言开发spark的wordcount程序（本地运行）public class WordCount_Java &#123; public static void main(String[] args) &#123; //1、创建SparkConf SparkConf sparkConf = new SparkConf().setAppName("WordCount_Java").setMaster("local[2]"); //2、创建JavaSparkContext JavaSparkContext jsc = new JavaSparkContext(sparkConf); //3、读取数据文件 JavaRDD&lt;String&gt; data = jsc.textFile("E:\\words.txt"); //4、切分每一行获取所有的单词 JavaRDD&lt;String&gt; words = data.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; public Iterator&lt;String&gt; call(String line) throws Exception &#123; String[] words = line.split(" "); return Arrays.asList(words).iterator(); &#125; &#125;); //5、每个单词计为1 （单词，1） JavaPairRDD&lt;String, Integer&gt; wordAndOne = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(word, 1); &#125; &#125;); //6、相同单词出现的1累加 JavaPairRDD&lt;String, Integer&gt; result = wordAndOne.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); //按照单词出现的次数降序排列 (单词，次数)-------&gt;(次数，单词).sortByKey-------&gt;(单词，次数) JavaPairRDD&lt;Integer, String&gt; reverseRDD = result.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123; public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123; return new Tuple2&lt;Integer, String&gt;(t._2, t._1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; sortedRDD = reverseRDD.sortByKey(false).mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123; public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; t) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(t._2, t._1); &#125; &#125;); //7、收集打印 List&lt;Tuple2&lt;String, Integer&gt;&gt; finalResult = sortedRDD.collect(); for (Tuple2&lt;String, Integer&gt; tuple : finalResult) &#123; System.out.println("单词："+tuple._1+" 次数："+tuple._2); &#125; //8、关闭 jsc.stop(); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
</search>
