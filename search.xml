<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[倒排索引]]></title>
    <url>%2Fblog%2F2019%2F03%2F21%2F%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[1.简介​ 倒排索引源于实际应用中需要根据属性的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引(inverted index)。带有倒排索引的文件我们称为倒排索引文件，简称倒排文件(inverted file)]]></content>
      <categories>
        <category>数据结构</category>
        <category>倒排索引</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性表]]></title>
    <url>%2Fblog%2F2019%2F03%2F12%2F%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>线性表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase的使用]]></title>
    <url>%2Fblog%2F2019%2F03%2F12%2FHbase%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Hbase官网：https://hbase.apache.org/‘ Hbase0.97中文官方文档：http://abloz.com/hbase/book.html#shell 1.利用命令行操作1.进入交互界面进入hbase命令行 1./hbase shell 显示hbase中的表 1list 2.创建表创建user表，包含info、data两个列族 123create 'user', 'info', 'data' #或者create 'user', &#123;NAME =&gt; 'info', VERSIONS =&gt; '3'&#125;，&#123;NAME =&gt; 'data'&#125; 3.插入数据向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsan 1put 'user', 'rk0001', 'info:name', 'zhangsan' 向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为female 1put 'user', 'rk0001', 'info:gender', 'female' 向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20 1put 'user', 'rk0001', 'info:age', 20 向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为picture 1put 'user', 'rk0001', 'data:pic', 'picture' 4.获取数据获取user表中row key为rk0001的所有信息 1get 'user', 'rk0001' 获取user表中row key为rk0001，info列族的所有信息 1get 'user', 'rk0001', 'info' 获取user表中row key为rk0001，info列族的name、age列标示符的信息 1get 'user', 'rk0001', 'info:name', 'info:age' 获取user表中row key为rk0001，info、data列族的信息 12345get 'user', 'rk0001', 'info', 'data'get 'user', 'rk0001', &#123;COLUMN =&gt; ['info', 'data']&#125;get 'user', 'rk0001', &#123;COLUMN =&gt; ['info:name', 'data:pic']&#125; 获取user表中row key为rk0001，列族为info，版本号最新5个的信息 12345get 'user', 'rk0001', &#123;COLUMN =&gt; 'info', VERSIONS =&gt; 2&#125;get 'user', 'rk0001', &#123;COLUMN =&gt; 'info:name', VERSIONS =&gt; 5&#125;get 'user', 'rk0001', &#123;COLUMN =&gt; 'info:name', VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]&#125; 获取user表中row key为rk0001，cell的值为zhangsan的信息 1get 'people', 'rk0001', &#123;FILTER =&gt; "ValueFilter(=, 'binary:zhangsan')"&#125; 获取user表中row key为rk0001，列标示符中含有a的信息 1get 'people', 'rk0001', &#123;FILTER =&gt; "(QualifierFilter(=,'substring:a'))"&#125; 插入数据，获取数据（过滤） 1234put 'user', 'rk0002', 'info:name', 'fanbingbing'put 'user', 'rk0002', 'info:gender', 'female'put 'user', 'rk0002', 'info:nationality', '中国'get 'user', 'rk0002', &#123;FILTER =&gt; "ValueFilter(=, 'binary:中国')"&#125; 5.查询数据查询user表中的所有信息 1scan 'user' 查询user表中列族为info的信息 12345scan 'user', &#123;COLUMNS =&gt; 'info'&#125;scan 'user', &#123;COLUMNS =&gt; 'info', RAW =&gt; true, VERSIONS =&gt; 5&#125;scan 'person', &#123;COLUMNS =&gt; 'info', RAW =&gt; true, VERSIONS =&gt; 3&#125; 查询user表中列族为info和data的信息 123scan 'user', &#123;COLUMNS =&gt; ['info', 'data']&#125;scan 'user', &#123;COLUMNS =&gt; ['info:name', 'data:pic']&#125; 查询user表中列族为info、列标示符为name的信息 1scan 'user', &#123;COLUMNS =&gt; 'info:name'&#125; 查询user表中列族为info、列标示符为name的信息,并且版本最新的5个 1scan 'user', &#123;COLUMNS =&gt; 'info:name', VERSIONS =&gt; 5&#125; 查询user表中列族为info和data且列标示符中含有a字符的信息 1scan 'user', &#123;COLUMNS =&gt; ['info', 'data'], FILTER =&gt; "(QualifierFilter(=,'substring:a'))"&#125; 查询user表中列族为info，rk范围是[rk0001, rk0003)的数据 scan ‘people’, {COLUMNS =&gt; ‘info’, STARTROW =&gt; ‘rk0001’, ENDROW =&gt; ‘rk0003’} 1 查询user表中row key以rk字符开头的 scan ‘user’,{FILTER=&gt;”PrefixFilter(‘rk’)”} 1 查询user表中指定范围的数据 scan ‘user’, {TIMERANGE =&gt; [1392368783980, 1392380169184]} 1 6.删除数据删除user表row key为rk0001，列标示符为info:name的数据 1delete 'people', 'rk0001', 'info:name' 删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据 1delete 'user', 'rk0001', 'info:name', 1392383705316 清空user表中的数据 1truncate 'people' 7.修改表结构首先停用user表 1disable 'user' 添加两个列族f1和f2 123alter 'people', NAME =&gt; 'f1'alter 'user', NAME =&gt; 'f2' 启用表 1enable 'user' 删除一个列族： alter ‘user’, NAME =&gt; ‘f1’, METHOD =&gt; ‘delete’ 或 alter ‘user’, ‘delete’ =&gt; ‘f1’ 123alter 'user', NAME =&gt; 'f1', METHOD =&gt; 'delete'#或 alter 'user', 'delete' =&gt; 'f1' 添加列族f1同时删除列族f2 1alter 'user', &#123;NAME =&gt; 'f1'&#125;, &#123;NAME =&gt; 'f2', METHOD =&gt; 'delete'&#125; 将user表的f1列族版本号改为5 alter ‘people’, NAME =&gt; ‘info’, VERSIONS =&gt; 5 启用表 1enable 'user' 删除表 12disable 'user'drop 'user' 查询数据 123456789101112131415161718 get 'person', 'rk0001', &#123;FILTER =&gt; "ValueFilter(=, 'binary:中国')"&#125;get 'person', 'rk0001', &#123;FILTER =&gt; "(QualifierFilter(=,'substring:a'))"&#125;scan 'person', &#123;COLUMNS =&gt; 'info:name'&#125;scan 'person', &#123;COLUMNS =&gt; ['info', 'data'], FILTER =&gt; "(QualifierFilter(=,'substring:a'))"&#125;scan 'person', &#123;COLUMNS =&gt; 'info', STARTROW =&gt; 'rk0001', ENDROW =&gt; 'rk0003'&#125;scan 'person', &#123;COLUMNS =&gt; 'info', STARTROW =&gt; '20140201', ENDROW =&gt; '20140301'&#125;scan 'person', &#123;COLUMNS =&gt; 'info:name', TIMERANGE =&gt; [1395978233636, 1395987769587]&#125;delete 'person', 'rk0001', 'info:name'alter 'person', NAME =&gt; 'ffff'alter 'person', NAME =&gt; 'info', VERSIONS =&gt; 10 2.Hbase代码开发1.Java实现增删改查导入pom依赖 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-common&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.过滤器查询]]></content>
      <categories>
        <category>大数据</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B叉数与B+叉数]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2FB%E5%8F%89%E6%95%B0%E4%B8%8EB%2B%E5%8F%89%E6%95%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据结构</category>
        <category>二叉树</category>
      </categories>
      <tags>
        <tag>B叉数</tag>
        <tag>B+叉数</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase集群搭建]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2FHbase%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.Hbase集群搭建 假设有node-1,node-2,node-3三台服务器 1.先搭建好Zookeeper跟Hadoop集群 2.下载Hbase安装包：https://hbase.apache.org/downloads.html 3.上传到服务器并解压 4.修改Hbase配置 要把hadoop的hdfs-site.xml和core-site.xml放到hbase/conf下 修改hbase-env.sh 1234#配置jdkexport JAVA_HOME=/export/servers/jdk#告诉hbase使用外部的zkexport HBASE_MANAGES_ZK=false 修改 hbase-site.xml 12345678910111213141516171819&lt;configuration&gt; &lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://node-1:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node-1:2181,node-2:2181,node-3:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 regionservers文件 vim regionservers 12node-2node-3 修改 backup-masters来指定备用的主节点 vim backup-masters 1node-2 配置hbase环境变量 vim /etc/profile 12export HBASE_HOME=/export/servers/hbase export PATH=$PATH:$HBASE_HOME/bin 5.拷贝hbase和环境变量到其他节点 1234scp -r hbase node-2:$PWDscp -r hbase node-3:$PWDscp /etc/profile node-2:/etcscp /etc/profile node-3:/etc 6.每个节点都重载环境变量 1source /etc/profile 2.启动跟关闭Hbase集群2-1.启动集群1.启动Zookeeper集群 用Zookeeper一键启动脚本 2.启动HDFS集群 1start-dfs.sh 3.启动Hbase在主节点node-1上运行 1start-hbase.sh 4.通过浏览器访问hbase管理页面 node-1:16010 node-2:16010 5.为保证集群的可靠性，要启动多个HMaster(比如说在node-2上再启动一个HMaster) 注意：使用jdk8的时候，出现了”Java HotSpot(TM) 64-Bit Server VM warning: ignoringoption MaxPermSize=256m; support was removed in 8.0”的红色标识。字面意思是MaxPermSize不需要我们配置了，所以我就按照它的方法把default VM arguments中MaxPermSize参数给删掉就不会出现上面的提示了。 2-2.关闭集群1.关闭Hbase集群 1stop-hbase.sh 2.关闭HDFS 1stop-dfs.sh 3.关闭Zookeeper 用ZK一键关闭脚本]]></content>
      <categories>
        <category>大数据</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>数据库</tag>
        <tag>搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkStreaming&Kafka&Flume整合]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2F%E6%95%B4%E5%90%88SparkStreaming%26Kafka%26Flume%2F</url>
    <content type="text"><![CDATA[整体流程 12345graph LR Flume集群--&gt;|Kafka集群Poll数据|Kafka集群((Kafka集群)) Kafka集群---|Spark集群Poll数据|Spark集群 1.Flume与Kafka整合1.安装部署好Flume与Kafka集群 2.配置flume-kafka.conf 12345678910111213141516171819202122#为我们的source channel sink起名a1.sources = r1a1.channels = c1a1.sinks = k1#指定我们的source收集到的数据发送到哪个管道a1.sources.r1.channels = c1#指定我们的source数据收集策略a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /export/servers/flumedataa1.sources.r1.deletePolicy = nevera1.sources.r1.fileSuffix = .COMPLETEDa1.sources.r1.ignorePattern = ^(.)*\\.tmp$a1.sources.r1.inputCharset = utf-8#指定我们的channel为memory,即表示所有的数据都装进memory当中a1.channels.c1.type = memory#指定我们的sink为kafka sink，并指定我们的sink从哪个channel当中读取数据a1.sinks.k1.channel = c1a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = testa1.sinks.k1.kafka.bootstrap.servers = node-1:9092,node-2:9092,node-3:9092a1.sinks.k1.kafka.flumeBatchSize = 20a1.sinks.k1.kafka.producer.acks = 1 3.启动Kafka集群 4.在Flume安装目录下启动Flume 1bin/flume-ng agent -n a1 -c conf -f conf/flume-kafka.conf -Dflume.root.logger=INFO,console 2.Kafka与Spark整合]]></content>
      <categories>
        <category>大数据</category>
        <category>整合</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>spark</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模板]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2F%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"></content>
      <categories>
        <category>模板</category>
      </categories>
      <tags>
        <tag>模板</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2F%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间复杂度&空间复杂度]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2F%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%26%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[1.时间复杂度1.定义​ 在计算机科学中，算法的时间复杂度是一个函数，它定性描述了该算法的运行时间。这是一个关于代表算法输入值的字符串的长度的函数。时间复杂度常用大O符号表述，不包括这个函数的低阶项和首项系数。 2.计算方法 用常数1取代运行时间中的所有加法常数 在修改后的运行次数函数中，只保留最高阶项 如果最高阶项存在且不是1，则去除与这个项想乘的常数 得到的最终结果是大O阶 例：1.常数阶 123int sum = 0; int n = 100;sum = (1+n)*n/2; 时间复杂度为 O(1) 2.线性阶 12345int sum = 0;int n = 100;for(int i = 1; i &lt; n; i++)&#123; sum = sum + i;&#125; 时间复杂度为O(n) 3.平方阶 123456int n = 100;for(int i = 1; i &lt; n; i++)&#123; for(int j = 0; j &lt; n;j++)&#123; system.out.print("Hello World"); &#125;&#125; 时间复杂度为O(n²) 4.对数阶 12345int i = 1;int n =100;while( i &lt; n )&#123; i = i * 2;&#125; 时间复杂度为O(logn) 2.空间复杂度​ 空间复杂度(Space Complexity)是对一个算法在运行过程中临时占用存储空间大小的量度，记做S(n)=O(f(n))。比如直接插入排序的时间复杂度是O(n^2),空间复杂度是O(1) 。而一般的递归算法就要有O(n)的空间复杂度了，因为每次递归都要存储返回信息。一个算法的优劣主要从算法的执行时间和所需要占用的存储空间两个方面衡量。 3.总结​ 对于一个算法，其时间复杂度和空间复杂度往往是相互影响的。当追求一个较好的时间复杂度时，可能会使空间复杂度的性能变差，即可能导致占用较多的存储空间；反之，当追求一个较好的空间复杂度时，可能会使时间复杂度的性能变差，即可能导致占用较长的运行时间。另外，算法的所有性能之间都存在着或多或少的相互影响。因此，当设计一个算法(特别是大型算法)时，要综合考虑算法的各项性能，算法的使用频率，算法处理的数据量的大小，算法描述语言的特性，算法运行的机器系统环境等各方面因素，才能够设计出比较好的算法。算法的时间复杂度和空间复杂度合称为算法的复杂度。]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>时间复杂度</tag>
        <tag>空间复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase介绍]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2FHbase%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Hbase官网：https://hbase.apache.org/ 1.Hbase简介 hbase是基于Google BigTable模型开发的，典型的key/value系统。是建立在hdfs之上，提供高可靠性、高性能、列存储、可伸缩、实时读写nosql的数据库系统。它是Apache Hadoop生态系统中的重要一员，主要用于海量结构化和半结构化数据存储 它介于nosql和RDBMS之间，仅能通过主键(row key)和主键的range来检索数据，仅支持单行事务(可通过hive支持来实现多表join等复杂操作)。 Hbase查询数据功能很简单，不支持join等复杂操作，不支持复杂的事务（行级的事务） 与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。 Hbase的特点 1.大：一个表可以有上十亿行，上百万列 2.无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列； 3.面向列:面向列(族)的存储和权限控制，列(族)独立检索。 4.稀疏：对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。 5.数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳 6.数据类型单一：Hbase中的数据都是字节数组 byte[]。 Hbase应用场景 2.集群结构 组件说明： Client： 包含访问Hbase的接口，并维护cache来加快对Hbase的访问，比如region的位置信息。 HMaster： 是hbase集群的主节点，可以配置多个，用来实现HA 为RegionServer分配region 负责RegionServer的负载均衡 HDFS上的垃圾文件回收 发现失效的RegionServer并重新分配其上的region 处理schema更新请求(改表) RegionServer： 维护region，处理对这些region的IO请求 负责切分在运行过程中变得过大的region 可以看到，client访问hbase上数据的过程并不需要master参与（寻址访问zookeeper和region server，数据读写访问regione server），master仅仅维护者table和region的元数据信息，负载很低。 HRegion： 分布式存储的最小单元。 HRegion由一个或者多个Store组成，每个store保存一个column family。每个store又由一个memStore和0至多个StoreFile组成 Zookeeper作用： 通过选举，保证任何时候，集群中只有一个活着的HMaster，HMaster与RegionServers 启动时会向ZooKeeper注册 存贮所有Region的寻址入口 实时监控Region server的上线和下线信息。并实时通知给HMaster 存储HBase的schema和table元数据 Zookeeper的引入使得HMaster不再是单点故障 3.表的结构下表为一个Hbase表结构： RowKey 列族(ColumnFamily)：BaseInfo 列族：ExtraInfo 000001 姓名：张三 \ 年龄：18 \ 性别：男 兴趣：踢球 \ 特长：数学 000002 姓名：李四 \ 年龄：19 兴趣：看电影 000003 姓名：王五 \ 年龄：20 兴趣：游戏（ver_01) \ 看电影(ver_02 1.行键(Row Key)： 与nosql数据库们一样,row key是用来检索记录的主键。访问hbase table中的行，只有三种方式： 通过单个row key访问 （select * from t1 where id=1） 通过row key的range （select * from t1 where id1） 全表扫描 （select * from t1 ） Rowkey行键 (Row key)可以是任意字符串(最大长度是 64KB，实际应用中长度一般为 10-100bytes)，在hbase内部，row key保存为字节数组。 Hbase会对表中的数据按照Rowkey字典序(byte order)排序存储，设计rowkey时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性) 注意： 字典序对int排序的结果是:1,10,1001,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用0作右填充。 行的一次读写是原子操作 (不论一次读写多少列)。这个设计决策能够使用户很容易的理解程序在对同一个行进行并发更新操作时的行为。 2.列族(ColumnFamily)：：- hbase表中的每个列，都归属与某个列族。列族是表的schema的一部分(而列不是)，必须在使用表之前定义。- 列名都以列族作为前缀。例如BaseInfo:姓名，BaseInfo:年龄 都属于 BaseInfo这个列族。- 访问控制、磁盘和内存的使用统计都是在列族层面进行的。- 列族越多，在取一行数据时所要参与IO、搜寻的文件就越多，所以，如果没有必要，不要设置太多的列族。一般设置2-3个比较合理。 3.时间戳(TimeStamp)： HBase中通过row和columns确定的为一个存贮单元称为cell。每个 cell都保存着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由hbase(在数据写入时自动 )赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。 为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase提供了两种数据版本回收方式： 保存数据的最后n个版本 保存最近一段时间内的版本（设置数据的生命周期TTL） 用户可以针对每个列族进行设置。 4.Cell： 由{row key, column( =&lt;family> + &lt;label>), version} 唯一确定的单元。 cell中的数据是没有类型的，全部是字节码形式存贮。 4.内部原理4.1 物理存储1.整体结构 Table中的所有行都按照row key的字典序排列。 Table 在行的方向上分割为多个Hregion。 region按大小分割的(默认10G)，每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，Hregion就会等分会两个新的Hregion。当table中的行不断增多，就会有越来越多的Hregion。 Hregion是Hbase中分布式存储和负载均衡的最小单元。最小单元就表示不同的Hregion可以分布在不同的HRegion server上。但一个Hregion是不会拆分到多个regionserver上的。 HRegion虽然是负载均衡的最小单元，但并不是物理存储的最小单元。事实上，HRegion由一个或者多个Store组成，每个store保存一个column family。每个Strore又由一个memStore和0至多个StoreFile组成。如上图 2.STORE FILE &amp; HFILE结构 HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数据块的起始点。 File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。 Data Index和Meta Index块记录了每个Data块和Meta块的起始点。 Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。 HFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构： 开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。 HFile分为六个部分： 1.Data Block 段：保存表中的数据，这部分可以被压缩 2.Meta Block 段：(可选的)–保存用户自定义的kv对，可以被压缩。 3.File Info 段：Hfile的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。 4.Data Block Index 段：Data Block的索引。每条索引的key是被索引的block的第一条记录的key。 5.Meta Block Index段 ：(可选的)–Meta Block的索引。 6.Trailer段：这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先 读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。 HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。 目标Hfile的压缩支持两种方式：Gzip，Lzo 3.Memstore(MemoryStore)与storefile 一个region由多个store组成，每个store包含一个列族的所有数据。 Store包括位于内存的memstore和位于硬盘的storefile。 memstore是为了更快的读写 写操作先写入memstore,当memstore中的数据量达到某个阈值，Hregionserver启动flashcache进程写入storefile,每次写入形成单独一个storefile。 当storefile的个数超过一定阈值后（默认参数hbase.hstore.blockingStoreFiles=10），多个storeFile会进行合并，当该region的所有store的storefile大小之和，即所有store的大小超过hbase.hregion.max.filesize=10G时，这个region会被拆分会把当前的region分割成两个，并由Hmaster分配给相应的region服务器，实现负载均衡。 客户端检索数据时，先在memstore找，找不到再找storefile。 4.HLog(WAL log) WAL 意为Write ahead log(http://en.wikipedia.org/wiki/Write-ahead_logging)，该机制用于数据的容错和恢复，Hlog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。 HLog文件就是一个普通的Hadoop Sequence File： HLog Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是”写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。 HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。 4.2 读写过程Hbase 0.98版本之前采用的寻址机制： ZooKeeper–&gt; -ROOT-(只有一个Region)–&gt; .META.–&gt; 用户表 Hbase 0.98版本之后采用的寻址机制： ZooKeeper–&gt; .META.–&gt; 用户表 读数据流程： Client发送请求，获取元数据所在的ResgionServer ZK集群返回Meta表所在的RegionServer Client请求Meta表，获取Rowkey所在的RegionServer Meta表返回Rowkey所在的RegionServer Clinet向RegionServer发起读数据请求 RegionServer返回结果（先从MemStore查，然后从布隆，最后从磁盘，） 1.先从MemStore查（因为内存里存的是最新的数据） 2.如果MemStore没有数据，而且开启了BloomFilter，则从BloomFilter中查询 3.如果BloomFilter中也没有，最后从StoreFile中查询 4.如果开启了BloomFilter，会先吧查询到的数据缓存到BloomFilter，再返回给Client 写数据流程： Client向ZK集群发送请求，获取Meta表所在的RegionServer ZK集群返回Meta表所在的RegionServer Client向Meta表发送请求，获取要写入数据的RegionServer Meta表返回RegionServer Client向RegionServer发送写入数据的请求 RegionServer先把数据写入到HLog（为了安全） RegionServer再把数据存入到MemStore RegionServer反馈给Client，数据写入成功 4.3 Region管理1.region分配： ​ 任何时刻，一个region只能分配给一个region server。master记录了当前有哪些可用的region server。以及当前哪些region分配给了哪些region server，哪些region还没有分配。当需要分配的新的region，并且有一个region server上有可用空间时，master就给这个region server发送一个装载请求，把region分配给这个region server。region server得到请求后，就开始对此region提供服务。 2.region server上线： ​ master使用zookeeper来跟踪region server状态。当某个region server启动时，会首先在zookeeper上的server目录下建立代表自己的znode。由于master订阅了server目录上的变更消息，当server目录下的文件出现新增或删除操作时，master可以得到来自zookeeper的实时通知。因此一旦region server上线，master能马上得到消息。 3.region server下线： ​ 当region server下线时，它和zookeeper的会话断开，zookeeper而自动释放代表这台server的文件上的独占锁。master就可以确定： 1.region server和zookeeper之间的网络断开了。 2.region server挂了。 无论哪种情况，region server都无法继续为它的region提供服务了，此时master会删除server目录下代表这台region server的znode数据，并将这台region server的region分配给其它还活着的同志。 4.4 Master工作机制 3.Master上线： 从zookeeper上获取唯一一个代表active master的锁，用来阻止其它master成为活着的master。 扫描zookeeper上的servermaster启动进行以下步骤: 父节点，获得当前可用的region server列表。 和每个region server通信，获得当前已分配的region和region server的对应关系。 扫描.META.region的集合，计算得到当前还未分配的region，将他们放入待分配region列表。 2.Master下线： ​ 由于master只维护表和region的元数据，而不参与表数据IO的过程，master下线仅导致所有元数据的修改被冻结(无法创建删除表，无法修改表的schema，无法进行region的负载均衡，无法处理region 上下线，无法进行region的合并，唯一例外的是region的split可以正常进行，因为只有region server参与)，表的数据读写还可以正常进行。因此master下线短时间内对整个hbase集群没有影响。 4.5 HBase容错机制 Master容错：Zookeeper重新选择一个新的Master，无Master过程中，数据读取仍照常进行，负载均衡无法进行。 RegionServer容错：定时向Zookeeper汇报心跳，如果一旦时间内未出现心跳，Master将该RegionServer上的Region重新分配到其他RegionServer上，失效服务器上“预写”日志由主服务器进行分割并派送给新的RegionServerZookeeper容错：Zookeeper是一个可靠地服务，一般配置3或5个Zookeeper实例# 5.高级应用## 5.1.建表高级属性下面几个shell 命令在hbase操作中可以起到很到的作用，且主要体现在建表的过程中，看下面几个create 属性1.BLOOMFILTER 默认是Row ​ 布隆过滤可以每列族单独启用。使用 HColumnDescriptor.setBloomFilterType(NONE | ROW | ROWCOL) 对列族单独启用布隆。 Default = ROW 对行进行布隆过滤。 对 ROW，行键的哈希在每次插入行时将被添加到布隆。 对 ROWCOL，行键 + 列族 + 列族修饰的哈希将在每次插入行时添加到布隆 使用方法: 1create 'table',&#123;BLOOMFILTER =&gt;'ROW'&#125; 启用布隆过滤可以节省读磁盘过程，可以有助于降低读取延迟 2.VERSIONS 默认是1 这个参数的意思是数据保留1个版本，如果我们认为我们的数据没有这么大的必要保留这么多，随时都在更新，而老版本的数据对我们毫无价值，那将此参数设为1 能节约2/3的空间 使用方法: 1create 'table',&#123;VERSIONS=&gt;'2'&#125; 附：MIN_VERSIONS =&gt; ‘0’是说在compact操作执行之后，至少要保留的版本 3.COMPRESSION 默认值是NONE 即不使用压缩 ​ 这个参数意思是该列族是否采用压缩，采用什么压缩算法 ​ 使用方法: 1create 'table',&#123;NAME=&gt;'info',COMPRESSION=&gt;'SNAPPY'&#125; ​ 建议采用SNAPPY压缩算法 ​ HBase中，在Snappy发布之前（Google 2011年对外发布Snappy），采用的LZO算法，目标是达到尽可能快的压缩和解压速度，同时减少对CPU的消耗； ​ 在Snappy发布之后，建议采用Snappy算法（参考《HBase: The Definitive Guide》），具体可以根据实际情况对LZO和Snappy做过更详细的对比测试后再做选择。 Algorithm % remaining Encoding Decoding GZIP 13.4% 21 MB/s 118 MB/s LZO 20.5% 135 MB/s 410 MB/s Zippy/Snappy 22.2% 172 MB/s 409 MB/s 如果建表之初没有压缩，后来想要加入压缩算法，可以通过alter修改schema 4.alter ​ 使用方法：如 修改压缩算法 123disable 'table'alter 'table',&#123;NAME=&gt;'info',COMPRESSION=&gt;'snappy'&#125; enable 'table' 但是需要执行major_compact ‘table’ 命令之后 才会做实际的操作。 5.TTL 默认是 2147483647 即:Integer.MAX_VALUE 值大概是68年 这个参数是说明该列族数据的存活时间，单位是s 这个参数可以根据具体的需求对数据设定存活时间，超过存过时间的数据将在表中不在显示，待下次major compact的时候再彻底删除数据. 注意的是TTL设定之后 MIN_VERSIONS=&gt;’0’ 这样设置之后，TTL时间戳过期后，将全部彻底删除该family下所有的数据，如果MIN_VERSIONS 不等于0那将保留最新的MIN_VERSIONS个版本的数据，其它的全部删除，比如MIN_VERSIONS=&gt;’1’ 届时将保留一个最新版本的数据，其它版本的数据将不再保存。 6.describe ‘table’ 这个命令查看了create table 的各项参数或者是默认值。 7.disable_all ‘toplist.*’ disable_all 支持正则表达式，并列出当前匹配的表的如下： ​ toplist_a_total_1001 ​ toplist_a_total_1002 ​ toplist_a_total_1008 ​ toplist_a_total_1009 ​ toplist_a_total_1019 ​ toplist_a_total_1035 ​ … ​ Disable the above 25 tables (y/n)? 并给出确认提示. 8.drop_all 这个命令和disable_all的使用方式是一样的 5.2.Hbase应用案例行键设计表结构设计 1、列族数量的设定 以用户信息为例，可以将必须的基本信息存放在一个列族，而一些附加的额外信息可以放在另一列族； 2、行键的设计 语音详单： 13877889988-20150625 13877889988-20150626 —-将需要批量查询的数据尽可能连续存放 CMS系统—-多条件查询 尽可能将查询条件关键词拼装到rowkey中，查询频率最高的条件尽量往前靠 20150230-zhangsan-category… 20150230-lisi-category… Category+20150230 20150230-zhangsan-category (每一个条件的值长度不同，可以通过做定长映射来提高效率) Id name age address 1 zhangsan 30 bj 2 lisi 40 sh 3 wangwu 50 bj 查询北京用户人群年龄 5.3.Rowkey的设计原则​ HBase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对HBase中的数据进行快速定位。 HBase中rowkey可以唯一标识一行记录，在HBase查询的时候，有以下几种方式： 通过get方式，指定rowkey获取唯一一条记录 通过scan方式，设置startRow和stopRow参数进行范围匹配 全表扫描，即直接扫描整张表中所有行记录 5.3.1 rowkey长度原则​ rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。建议越短越好，不要超过16个字节。 原因如下： 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率. MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。 5.3.2 rowkey散列原则​ 如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。 5.3.3 rowkey唯一原则​ 必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。 5.4 热点问题​ HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。 ​ 热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 ​ 设计良好的数据访问模式以使集群被充分，均衡的利用。为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。下面是一些常见的避免热点的方法以及它们的优缺点: 5.4.1 加盐​ 这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。 5.4.2 哈希​ 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。 5.4.3 反转​ 反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。 ​ 反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题 135xxxx1023 → 3201xxxx531 136xxxx9301 → 1039xxxx631 5.4.4 时间戳反转​ 一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value -timestamp 追加到key的末尾，例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。 5.4.5 其他 尽量减少行键和列族的大小在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，这个时候它们将会占用大量的存储空间。 列族尽可能越短越好，最好是一个字符。冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好。]]></content>
      <categories>
        <category>大数据</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka集群搭建]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2FKafka%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Kafka集群搭建假设有node-1,node-2,node-3三台服务器 1.搭建一个ZK集群 2.下载对应的安装包：https://kafka.apache.org/downloads 3.上传到node-1服务器，并解压到指定目录 在这里指定安装目录为：”/export/servers” 可以重命名目录，在这里改为 “kafka” 4.修改配置文件 进入到kafka安装目录下的config目录 vim server.properties 1234567891011121314#指定每台节点的brokerid 必须唯一，不能够重复broker.id=0#指定这个broker的主机名host.name=node-1#指定topic的数据是否可以删除，默认等于false表示不可以，改为true表示可以delete.topic.enable=true#指定topic的数据存放目录log.dirs=/export/servers/kafka/kafka-logs#指定依赖的zk集群地址zookeeper.connect=node-1:2181,node-2:2181,node-3:2181 5.配置kafka环境变量 vim /etc/profile 12export KAFKA_HOME=/export/servers/kafkaexport PATH=$PATH:$KAFKA_HOME/bin 6.分发到其他服务器上 12345scp -r kafka node-2:/export/serversscp -r kafka node-3:/export/serversscp /etc/profile node-2:/etcscp /etc/profile node-3:/etc 7.修改node-2和node-3上的配置 node-2需要修改的部分 12345#指定每台节点的brokerid 必须唯一，不能够重复broker.id=1#指定这个broker的主机名host.name=node-2 node-3需要修改的部分 12345#指定每台节点的brokerid 必须唯一，不能够重复broker.id=2#指定这个broker的主机名host.name=node-3 8.让所有kafka节点的环境变量生效 在所有的节点执行1source /etc/profile Kafka集群启动&amp;关闭1.启动集群1.可以手动启动每一个节点 12#在bin目录下nohup kafka-server-start.sh config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp; 2.可以编写一键启动脚本 1234567891011#!/bin/bash#指定Kafka集群的IP,并利用for循环依次启动for host in &#123;node-1,node-2,node-3&#125;do #1.重载环境变量 #2.调用bin目录下的kafka-server-start.sh 启动Kafka #3.丢弃日志信息 ssh $host "source /etc/profile;/export/servers/kafka/bin/kafka-server-start.sh \ /export/servers/kafka/config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;" echo "$host kafaka is running"done 2.关闭集群1.可以手动关闭每一个节点 12#在bin目录下kafka-server-stop.sh 这个脚本有一定的问题（centos6.x有问题，如果是centos 7.x是没有问题）可以用一下方法修改 1234#vi kafka-server-stop.sh 将PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')#改为PIDS=$(ps ax | grep -i 'kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;') 2.可以编写shell脚本一键关闭 123456#!/bin/bashfor host in &#123;node-1,node-2,node-3&#125;do ssh $host "/export/servers/kafka/bin/kafka-server-stop.sh stop" echo "$host kafaka is stop"done]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka集群搭建</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka的使用]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2FKafka%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.kafka的命令行的管理使用现在有三台服务器组成的Kafka集群（node-1,node-2,node-3） 官网操作手册：https://kafka.apache.org/quickstart 1.创建topic1234567891011121314kafka-topics.sh \--create \--topic KafkaTest \--partitions 3 \--replication-factor 2 \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--create:表示创建--topic：指定要创建的topic名称--partitions： 指定topic的分区数--replication-factor：指定每一个分区的副本数--zookeeper ：指定依赖的zk集群地址 2.显示所有的topic1234567kafka-topics.sh \--list \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--list:查看所有的topic信息--zookeeper ：指定依赖的zk集群地址 3.模拟生产者1234567kafka-console-producer.sh \--broker-list node-1:9092,node-2:9092,node-3:9092 \--topic KafkaTest#参数说明--topic：指定topic的名称--broker-list ：指定kafka集群地址 4.模拟消费者123456789kafka-console-consumer.sh \--from-beginning \--topic KafkaTest \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--bootstrap-server:指定kafka集群地址--topic:指定topic的名称--from-beginning:从最开始的第一条数据开始消费 5.删除topic123456789kafka-topics.sh \--delete \--topic test \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--delete ：表示要删除topic--topic：指定topic的名称--zookeeper：指定zookeeper的地址 注意事项 如果缺少所需要的参数，会有错误提示。根据错误提示添加所需要的参数 12#例如少写了 --zookeeper参数，会提示Missing required argument "[zookeeper]" 2.Kafka的API1.导入所依赖的jar包12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 2.Kafka生产者代码1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;//TODO：开发一个Kafka生产者代码public class KafkaProducerDemo1 &#123; public static void main(String[] args) &#123; Properties props = new Properties(); //todo:指定kafka集群地址 props.put("bootstrap.servers", "node-1:9092,node-2:9092,node-3:9092"); //todo:消息的确认机制 props.put("acks", "all"); //todo:重试的次数 props.put("retries", 0); //todo:设置批量的导入数据的大小 props.put("batch.size", 16384); //todo:表示延迟时间 props.put("linger.ms", 1); //todo:缓冲区的内存大小 props.put("buffer.memory", 33554432); //todo:序列化key值 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //todo:序列化value值 props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //todo:增加ack机制 props.put("acks", "all"); //todo:添加自定义的分区规则 props.put("partitioner.class", "MyPartitioner"); //new KafkaProducer&lt;String, String&gt;(props) //todo:7.有2个String类型的泛型，第一个String表示消息的key的类型，key是消息的标识，第二个String表示消息的内容 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 0; i &lt; 100; i++) //第一个参数表示topic名称，第二个参数表示消息的key，第三个参数表示消息的内容 producer.send(new ProducerRecord&lt;String, String&gt;("test", Integer.toString(i),"hadoop spark")); producer.close(); &#125;&#125; 3.Kafka消费者代码自动提交偏移量 12345678910111213141516171819202122232425262728293031323334import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;//todo:开发一个kafka消费者代码public class KafkaConsumerDemo1 &#123; public static void main(String[] args) &#123; Properties props = new Properties(); //todo:指定kafka集群地址 props.put("bootstrap.servers", "node-1:9092,node-2:9092,node-3:9092"); //todo:消费者组id props.put("group.id", "test"); //todo:消息的偏移量是自动提交 props.put("enable.auto.commit", "true"); //todo:消息自动提交偏移量的时间间隔 props.put("auto.commit.interval.ms", "1000"); //todo:key的反序列化 props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); //todo:value的反序列化 props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //todo:指定要消费的topic名称 consumer.subscribe(Arrays.asList("test")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) //System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); System.out.println("offset:"+record.offset()+" key:"+record.key()+" value："+record.value()+" partition:"+record.partition()); &#125; &#125;&#125; 4.Kafka的自定义分区策略4.1直接指定分区12//第一个参数表示topic名称，第二个参数表示分区号，第三个参数表示消息的key，第四个参数表示消息的内容kafkaProducer.send(new ProducerRecord&lt;String, String&gt;("testpart",1,"0","value"+i)); 4.2自定义分区123456789101112131415161718192021222324252627282930313233import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import java.util.Map;//自定义分区public class MyPartitioner implements Partitioner&#123; /** * * @param topic topic名称 * @param key 消息的key * @param keyBytes 消息的key的字节数组 * @param value 消息的内容 * @param valueBytes 消息的内容字节数组 * @param cluster kafka集群对象 * @return */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; // key.hashcode % 分区数=分区号-----------hashPartitioner int partitions = cluster.partitionsForTopic(topic).size(); //(-2,-1,0,1,2) int partition = Math.abs(key.hashCode() % partitions); return partition; &#125; public void close() &#123; &#125; public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 5.Kafka12]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper的使用]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2FZookeeper%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.启动跟关闭 现在有三个Zookeeper集群节点，分别为node-1，node-2，node-3 1-1.手动启动&amp;关闭 在每个节点中进入zookeeper节点，执行以下命令 启动: 1bin/zkServer.sh start 关闭: 1bin/zkServer.sh stop 1-2.整体启动&amp;关闭集群启动脚本: 12345#!/bin/bashfor host in &#123;node-1,node-2,node-3&#125;do ssh $host "source /etc/profile;/export/servers/zookeeper/bin/zkServer.sh start"done 集群关闭脚本: 12345#!/bin/bashfor host in &#123;node-1,node-2,node-3&#125;do ssh $host "/export/servers/zookeeper/bin/zkServer.sh stop"done 2.Zookeeper的Shell操作2-1.客户端连接&amp;退出连接客户端：在Zookeeper安装目录的bin目录下，运行 zkCli.sh –serverIp 1bin/zkCli.sh –node-1 输入 help，输出 zk shell 提示 退出客户端：quit 2-2.创建节点create [-s] [-e] path data acl 参数说明： create：创建 [-s]：指序列化节点 [-e]：指临时节点 path：创建的路径 data：节点的数据 acl：指权限控制 创建永久节点： 1create /test 123 创建永久序列化节点： 1create -s /test 123 创建临时节点： 1create -e /test-temp test-temp 创建临时序列化节点： 1create -e -s /test-temp test-temp 2-3.读取节点​ 与读取相关的命令有 ls 命令和 get 命令，ls 命令可以列出 Zookeeper 指定节点下的所有子节点，只能查看指定节点下的第一级的所有子节点；get 命令可以获取 Zookeeper 指定节点的数据内容和属性信息。 ls path [watch] get path [watch] ls2 path [watch] 2-4.更新节点2-5.删除节点set path data [version] 2-6.其他命令3.Java API操作]]></content>
      <categories>
        <category>大数据</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper介绍]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2FZookeepr%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Zookeeper官网：https://zookeeper.apache.org/ 1.简介​ Zookeeper 是一个分布式协调服务的开源框架。主要用来解决分布式集群中应用系统的一致性问题，例如怎样避免同时操作同一数据造成脏读的问题。 ​ ZooKeeper 本质上是一个的分布式的小文件存储系统。 提供基于类似于文件系统的目录树方式的数据存储，并且可以对树中的节点进行有效管理。从而用来维护和监控你存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。 ​ 诸如： 统一命名服务(dubbo)、分布式配置管理(solr的配置集中管理)、分布式消息队列（sub/pub）、分布式锁、分布式协调等功能。 2.架构 Leader： Zookeeper 集群工作的核心 事务请求（写操作） 的唯一调度和处理者，保证集群事务处理的顺序性； 集群内部各个服务器的调度者。 对于 create， setData， delete 等有写操作的请求，则需要统一转发给leader 处理， leader 需要决定编号、执行操作，这个过程称为一个事务。 Follower： 处理客户端非事务（读操作） 请求 转发事务请求给 Leader； 参与集群 Leader 选举投票 2n-1台可以做集群投票。 此外，针对访问量比较大的 zookeeper 集群， 还可新增Observer角色。 Observer: 观察者角色，观察 Zookeeper 集群的最新状态变化并将这些状态同步过来，其对于非事务请求可以进行独立处理，对于事务请求，则会转发给 Leader服务器进行处理。 不会参与任何形式的投票只提供非事务服务，通常用于在不影响集群事务处理能力的前提下提升集群的非事务处理能力。说白了就是增加并发的读请求 3.特性 全局数据一致：每个 server 保存一份相同的数据副本， client 无论连接到哪个 server，展示的数据都是一致的，这是最重要的特征； 可靠性：如果消息被其中一台服务器接受，那么将被所有的服务器接受。 顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息 a 在消息 b 前发布，则在所有Server 上消息 a 都将在消息 b 前被发布；偏序是指如果一个消息 b 在消息 a 后被同一个发送者发布， a 必将排在 b 前面。 数据更新原子性：一次数据更新要么成功（半数以上节点成功），要么失败，不存在中间状态； 实时性：Zookeeper 保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。 4.数据模型​ ZooKeeper 的数据模型，在结构上和标准文件系统的非常相似，拥有一个层次的命名空间，都是采用树形层次结构，ZooKeeper 树中的每个节点被称为— Znode。和文件系统的目录树一样，ZooKeeper 树中的每个节点可以拥有子节点。 但也有不同之处： Znode 兼具文件和目录两种特点。既像文件一样维护着数据、元信息、ACL、 时间戳等数据结构，又像目录一样可以作为路径标识的一部分，并可以具有 子 Znode。用户对 Znode 具有增、删、改、查等操作（权限允许的情况下）。 Znode 具有原子性操作，读操作将获取与节点相关的所有数据，写操作也将 替换掉节点的所有数据。另外，每一个节点都拥有自己的 ACL(访问控制列表)，这个列表规定了用户的权限，即限定了特定用户对目标节点可以执行的操作。 Znode 存储数据大小有限制。ZooKeeper 虽然可以关联一些数据，但并没有 被设计为常规的数据库或者大数据存储，相反的是，它用来管理调度数据， 比如分布式应用中的配置文件信息、状态信息、汇集位置等等。这些数据的 共同特性就是它们都是很小的数据，通常以 KB 为大小单位。ZooKeeper 的服 务器和客户端都被设计为严格检查并限制每个 Znode 的数据大小至多 1M，常规使用中应该远小于此值。 Znode 通过路径引用，如同 Unix 中的文件路径。路径必须是绝对的，因此他 们必须由斜杠字符来开头。除此以外，他们必须是唯一的，也就是说每一个 路径只有一个表示，因此这些路径不能改变。在 ZooKeeper 中，路径由 Unicode 字符串组成，并且有一些限制。字符串”/zookeeper”用以保存管理 信息，比如关键配额信息。 4-1.数据结构 图中的每个节点称为一个Znode ，每个 Znode 由 3 部分组成: stat：此为状态信息, 描述该 Znode 的版本, 权限等信息 data：与该 Znode 关联的数据 children：该 Znode 下的子节点 4-2.节点类型​ Znode 有两种类型，分别为临时节点和永久节点。节点的类型在创建时即被确定，并且不能改变。 临时节点：该节点的生命周期依赖于创建它们的会话。一旦会话结束，临时 节点将被自动删除，当然可以也可以手动删除。临时节点不允许拥有子节点。永久节点：该节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，他们才能被删除。—​ Znode 还有一个序列化的特性，如果创建的时候指定的话，该 Znode 的名字后面会自动追加一个不断增加的序列号。序列号对于此节点的父节点来说是唯一的，这样便会记录每个子节点创建的先后顺序。它的格式为“%10d”(10 位数字，没有数值的数位用 0 补充，例如“0000000001”)。这样便会存在四种类型的 Znode 节点，分别对应： PERSISTENT：永久节点 EPHEMERAL：临时节点 PERSISTENT_SEQUENTIAL：永久节点、序列化 EPHEMERAL_SEQUENTIAL：临时节点、序列化## 4-3.节点属性每个 znode 都包含了一系列的属性，通过命令 get，可以获得节点的属性cZxid ：Znode 创建的事务 id。ctime：节点创建时的时间戳.mZxid ：Znode 被修改的事务 id，即每次对 znode 的修改都会更新 mZxid。* 对于 zk 来说，每次的变化都会产生一个唯一的事务 id，zxid（ZooKeeper Transaction Id）。通过 zxid，可以确定更新操作的先后顺序。例如，如果 zxid1小于 zxid2，说明 zxid1 操作先于 zxid2 发生，zxid 对于整个 zk 都是唯一的，即使操作的是不同的 znode。mtime：节点最新一次更新发生时的时间戳.cversion ：子节点的版本号。当 znode 的子节点有变化时，cversion 的值就会增加 1。dataVersion：数据版本号，每次对节点进行 set 操作，dataVersion 的值都会增加 1（即使设置的是相同的数据），可有效避免了数据更新时出现的先后顺序问题。aclVersion ：ACL 的版本号。ephemeralOwner：如果该节点为临时节点, ephemeralOwner 值表示与该节点绑定的 session id. 如果不是, ephemeralOwner 值为 0.在 client 和 server 通信之前,首先需要建立连接,该连接称为 session。连接建立后,如果发生连接超时、授权失败,或者显式关闭连接,连接便处于 CLOSED状态, 此时 session 结束。# 5.Watch机制## 5-1.watch机制简介​ ZooKeeper 提供了分布式数据发布/订阅功能，一个典型的发布/订阅模型系统定义了一种一对多的订阅关系，能让多个订阅者同时监听某一个主题对象，当这个主题对象自身状态变化时，会通知所有订阅者，使他们能够做出相应的处理。 ZooKeeper 中，引入了 Watcher 机制来实现这种分布式的通知功能 。ZooKeeper 允许客户端向服务端注册一个 Watcher 监听，当服务端的一些事件触发了这个 Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能。触发事件种类很多，如：节点创建，节点删除，节点改变，子节点改变等。总的来说可以概括 Watcher 为以下三个过程：1. 客户端向服务端注册 Watcher2. 服务端事件发生触发 Watcher3. 客户端回调 Watcher 得到触发事件情况## 5-2.watch机制特点一次性触发： 事件发生触发监听，一个 watcher event 就会被发送到设置监听的客户端，这种效果是一次性的，后续再次发生同样的事件，不会再次触发。 事件封装： ZooKeeper 使用 WatchedEvent 对象来封装服务端事件并传递。 WatchedEvent 包含了每一个事件的三个基本属性： 通知状态（keeperState） 事件类型（EventType） 和节点路径（path） Event 异步发送 watcher 的通知事件从服务端发送到客户端是异步的。 先注册再触发： Zookeeper 中的 watch 机制，必须客户端先去服务端注册监听，这样事件发送才会触发监听，通知给客户端。 5-3通知状态和事件类型​ 同一个事件类型在不同的通知状态中代表的含义有所不同，下表列举了常见的通知状态和事件类型。 KeeperStateEventType触发条件说明SyncConnected(3)None(-1)客户端与服务器成功建立会话此时客户端和服务端处于连接状态NodeCreated(1)Watcher监听的对应数据节点被创建NodeDeleted(2)Watcher监听的对应数据节点被删除NodeDataChanged(3)Watcher监听的对应数据节点的数据内容发生变更NodeChildrenChanged(4)Watcher监听的对应数据节点的子节点列表发生变更Disconnected(0)None(-1)客户端与服务器断开连接此时客户端和服务端处于断开连接状态Expired(-112)None(-1)会话超时此时客户端会话失效，通常同时也会收到SessionExpiredException异常AuthFailed(4)None(-1)通常有两种情况：使用错误的scheme进行权限检查，SASL权限检查失败通常也会收到AuthFailedException异常]]></content>
      <categories>
        <category>大数据</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka介绍]]></title>
    <url>%2Fblog%2F2019%2F03%2F09%2FKafka%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1.Kafka概述Kafka官网：https://kafka.apache.org/ Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。 Kafka最初是由LinkedIn开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高吞吐量、低延迟的平台。 Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。 Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性 2.Kafka集群架构 3.kafka组件介绍Topic ：消息根据Topic进行归类 Producer：发送消息者 Consumer：消息接受者 broker：每个kafka实例(server) Zookeeper：依赖集群保存meta信息。 Topics组件Topic：一类消息，每个topic将被分成多个partition(区)，在集群的配置文件中配置。 partition：在存储层面是逻辑append log文件，包含多个segment文件。 Segement：消息存储的真实文件，会不断生成新的。 offset：每条消息在文件中的位置（偏移量）。offset为一个long型数字，它是唯一标记一条消息。 Partition1.在存储层面是逻辑append log文件，每个partition有多个segment组成。 2.任何发布到此partition的消息都会被直接追加到log文件的尾部。 3.每个partition在内存中对应一个index列表，记录每个segment中的第一条消息偏移。这样查找消息的时候，先在index列表中定位消息位置，再读取文件，速度块。 4.发布者发到某个topic的消息会被均匀的分布到多个part上，broker收到发布消息往对应part的最后一个segment上添加该消息。 3.Kafka存储机制存储机制概述 Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。每个topic又可以分成几个不同的partition，每个partition存储一部分Message。 Partition是以文件的形式存储在文件系统中比如，创建了一个名为”test”的topic，其有3个partition，那么在Kafka的数据目录中(由配置文件中的log.dirs指定的)中就会有这样5个目录: test-0，test-1，test-2其命名规则为&lt;topic_name&gt;-&lt;partition_id&gt;，里面存储的分别就是这3个partition的数据。 每一个partition目录下的文件被平均切割成大小相等（默认一个文件是1G，可以手动去设置）的数据文件，每一个数据文件都被称为一个段（segment file），但每个段消息数量不一定相等，这种特性能够使得老的segment可以被快速清除。默认保留7天的数据。 每次满1G后，在写入到一个新的文件中。 另外每个partition只需要支持顺序读写就可以。如上图所示： 首先00000000000000000000.log是最早产生的文件，该文件达到1G后又产生了新的00000000000009084544.log文件，新的数据会写入到这个新的文件里面。这个文件到达1G后，数据又会写入到下一个文件中。也就是说它只会往文件的末尾追加数据，这就是顺序写的过程，生产者只会对每一个partition做数据的追加（写操作）。 12· 数据消费问题讨论问题：如何保证消息消费的有序性呢？比如说生产者生产了0到100个商品，那么消费者在消费的时候按照0到100这个从小到大的顺序消费， 那么kafka如何保证这种有序性呢？ 难度就在于，生产者生产出0到100这100条数据之后，通过一定的分组策略存储到broker的partition中的时候， 比如0到10这10条消息被存到了这个partition中，10到20这10条消息被存到了那个partition中，这样的话，消息在分组存到partition中的时候就已经被分组策略搞得无序了。 那么能否做到消费者在消费消息的时候全局有序呢？ 遇到这个问题，我们可以回答，在大多数情况下是做不到全局有序的。但在某些情况下是可以做到的。比如我的partition只有一个，这种情况下是可以全局有序的。 那么可能有人又要问了，只有一个partition的话，哪里来的分布式呢？哪里来的负载均衡呢？ 所以说，全局有序是一个伪命题！全局有序根本没有办法在kafka要实现的大数据的场景来做到。但是我们只能保证当前这个partition内部消息消费的有序性。 结论：一个partition中的数据是有序的吗？ 回答：间隔有序，不连续。 Segment文件Segment file是什么？ ​ 生产者生产的消息按照一定的分组策略被发送到broker中partition中的时候，这些消息如果在内存中放不下了，就会放在文件中。 ​ partition在磁盘上就是一个目录，该目录名是topic的名称加上一个序号，在这个partition目录下，有两类文件，一类是以log为后缀的文件，一类是以index为后缀的文件，每一个log文件和一个index文件相对应，这一对文件就是一个segment file，也就是一个段。 ​ 其中的log文件就是数据文件，里面存放的就是消息，而index文件是索引文件，索引文件记录了元数据信息 Segment文件特点 ​ segment文件命名的规则：partition全局的第一个segment从0（20个0）开始，后续的每一个segment文件名是上一个segment文件中最后一条消息的offset值。 ​ 那么这样命令有什么好处呢？假如我们有一个消费者已经消费到了368776（offset值为368776），那么现在我们要继续消费的话，怎么做呢？ ​ 看上图，分2个步骤，第1步是从所有文件log文件的的文件名中找到对应的log文件，第9100000条数据位于上图中的“00000000000009084544.log”这个文件中， ​ 这一步涉及到一个常用的算法叫做“二分查找法”（假如我现在给你一个offset值让你去找，你首先是将所有的log的文件名进行排序，然后通过二分查找法进行查找， ​ 很快就能定位到某一个文件，紧接着拿着这个offset值到其索引文件中找这条数据究竟存在哪里）；第2步是到index文件中去找第9100000条数据所在的位置。 ​ 索引文件（index文件）中存储这大量的元数据，而数据文件（log文件）中存储这大量的消息。 ​ 索引文件（index文件）中的元数据指向对应的数据文件（log文件）中消息的物理偏移地址。 kafka如何快速查询数据 上图的左半部分是索引文件，里面存储的是一对一对的key-value，其中key是消息在数据文件（对应的log文件）中的编号，比如“1,3,6,8……”， 分别表示在log文件中的第1条消息、第3条消息、第6条消息、第8条消息……，那么为什么在index文件中这些编号不是连续的呢？ 这是因为index文件中并没有为数据文件中的每条消息都建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。 这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。 但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。 其中以索引文件中元数据3,497为例，其中3代表在右边log数据文件中从上到下第3个消息(在全局partiton表示第368772个消息)， 其中497表示该消息的物理偏移地址（位置）为497。 存储机制总结Kafka高效文件存储设计特点：• Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。• 通过索引信息可以快速定位message。• 通过index元数据全部映射到memory，可以避免segmentfile的IO磁盘操作。• 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 4.Kafka的如何保证 数据不丢失4.1生产者如何保证数据的不丢失 Kafka的ack机制：在kafka发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到。 如果是同步模式：ack机制能够保证数据的不丢失 在config目录中的producer.properties设置 123#同步模式producer.type=sync request.required.acks=1 其中 acks=0 表示不等待broker的确认信息，最小延迟acks=1 表示partition leader接收到确认信息后，broker就返回成功。不管其他的副本是否成功（Replica）acks=all 或者 acks=-1 表示只有producer全部写成功的时候，才算成功 如果是异步模式：通过buffer来进行控制数据的发送，有两个值来进行控制，时间阈值与消息的数量阈值，如果buffer满了数据还没有发送出去，如果设置的是立即清理模式，风险很大，一定要设置为阻塞模式。 结论：producer有丢数据的可能，但是可以通过配置保证消息的不丢失 123456producer.type=async request.required.acks=1 queue.buffering.max.ms=5000 queue.buffering.max.messages=10000 queue.enqueue.timeout.ms = -1 batch.num.messages=200 4.2 消费者如何保证数据的不丢失通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的offset数值，下次继续消费的时候，接着上次的offset进行消费即可。]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark基础]]></title>
    <url>%2Fblog%2F2019%2F03%2F09%2Fspark%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[spark基础1、目标 1、掌握spark相关概念 2、掌握搭建一个spark集群 3、掌握开发简单的spark应用程序 2、spark概述 2.1 spark是什么 Apache Spark™ is a unified analytics engine for large-scale data processing. apache的spark是一个针对于大规模数据处理的统一分析引擎 spark是基于内存计算的分布式分析引擎，计算速度非常快，这里仅仅只涉及到数据的计算，并没有涉及到数据的存储，后期再使用spark的时候，就需要对接一些外部的数据源（比如HDFS） 2.2 为什么要学习spark1由于spark的处理速度比mapreduce快很多，很受企业青睐，所以我今天就重点学习它 2.3 spark框架的四大特性 1、速度快 spark比mapreduce处理任务在内存中快100倍，在磁盘中快10倍 spark比mapreduce速度快的2个主要原因 12345（1） mapreduce任务每一次job的输出结果只能够保存在磁盘中，后续有其他的job需要依赖于前面job的输出结果，这个时候只能够进行大量的磁盘io操作，获取得到。spark任务每一次的job输出结果可以直接保存在内存中，后续有其他的job需要依赖于前面job的输出结果，这个时候就可以直接从内存中获取得到。大大减少磁盘io操作，最后整体上提升性能。 hivesql: select name,age,sex from (select * from user where age &gt;30) (2) mapreduce任务它是以进程的方式运行在yarn中，比如一个mapreduce任务中，有100个MapTask,这个时候就需要开启100个进程去运行着100个task；spark任务的它是以线程的方式运行在进程中，比如一个spark任务中还是有100个task，后期再运行的时候，可以极端一点，开启一个进程，在这一个进程中运行100个线程即可。 开启一个进程跟开启一个线程的代价肯定是不一样，开启一个进程需要的时间远远高于开启一个线程需要的时间。 2、易用性 可以快速开发一个spark应用程序，通过java、scala、python、R、SQL等不同的语言进行代码开发 3、通用性 spark框架不在是一个简单的框架，它发展成一个spark的生态系统，它是包括了很多不同的子项目 Sparksql sparkStreaming Mlib Graphx 4、兼容性 spark程序就是一个计算任务，哪里可以给当前这个任务提供计算资源，我们就可以把spark程序提交到哪里去运行 standAlone 它是spark集群自带的模式，整个任务的资源分配由spark集群中的老大Master节点负责 yarn 可以把spark程序提交到yarn中去运行，整个任务的资源分配由yarn中的老大ResouceManager mesos 它也是一个类似于yarn的开源的资源调度框架 3、spark集群安装部署 1、下载对应的安装包 http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.1.3/spark-2.1.3-bin-hadoop2.7.tgz spark-2.1.3-bin-hadoop2.7.tgz 2、规划安装目录 /export/servers 3、上传安装包到服务器中 4、解压安装包到指定的规划目录 tar -zxvf spark-2.1.3-bin-hadoop2.7.tgz -C /export/servers 5、重命名解压目录 mv spark-2.1.3-bin-hadoop2.7 spark 6、修改配置文件 进入到spark的安装目录有一个conf文件夹 vim spark-env.sh ( mv spark-env.sh.template spark-env.sh) 123456#配置java环境变量export JAVA_HOME=/export/servers/jdk#指定master的地址export SPARK_MASTER_HOST=node1#指定master的端口export SPARK_MASTER_PORT=7077 vim slaves (mv slaves.template slaves) 123#指定哪些节点是workernode2node3 7、配置spark环境变量 vim /etc/profile 12export SPARK_HOME=/export/servers/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 8、分发spark目录和环境变量 12345scp -r spark node2:/export/serversscp -r spark node3:/export/serversscp /etc/profile node2:/etcscp /etc/profile node3:/etc 9、让所有spark节点的环境变量生效 可以在所有节点执行 source /etc/profile 4、spark集群的启动和停止 1、启动spark集群 在主节点执行脚本 sbin/start-all.sh 首先在主节点启动了一个Master进程，它是整个spark集群的老大，负责任务的资源分配 通过slaves文件执行的worker节点来分别启动worker进程 2、停止spark集群 在主节点执行脚本 sbin/stop-all.sh 5、spark集群的web管理界面 1、启动好spark集群之后 可以访问地址 master主机名:8080 可以看到spark集群的所有信息 spark集群的总的资源信息 spark集群已经使用的资源信息 spark集群还剩的资源信息 spark集群中每一个worker的相关信息 spark集群中正在运行的任务信息 spark集群中已经完成的任务信息 6、基于zk构建高可用的spark集群 1、事先搭建一个zk集群 2、修改配置 vim spark-env.sh 12345#把之前手动指定哪一个节点是活着的master这个参数注释掉#export SPARK_MASTER_HOST=node1#引入zk，构建高可用的spark集群export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 3、分发文件到其他机器 12scp spark-env.sh node2:/export/servers/spark/confscp spark-env.sh node3:/export/servers/spark/conf 4、先启动zk 5、启动spark集群 1、可以在任意一台机器来启动一个脚本（前提条件：实现任意2台机器之间的ssh免密登录） sbin/start-all.sh 2、它会在当前机器启动一个Master进程（活着的master） 3、整个spark集群中的worker进程由slaves文件决定 4、在其他机器单独启动master sbin/start-master.sh master恢复逻辑 12345678910首先基于引入了zk集群，这个时候整个spark集群中有很多个master，其中一个master被选举成活着的master，它提供服务，后期给任务分配资源。还有其他多个master被选举成备用的master（standBy）,它不提供服务。当前活着的master挂掉了，首先zk会感知到，接下来在所有处于standBy中的master进行选举，最后生成一个活着的master，后期需要从zk集群中读取保存了spark集群的元数据节点spark。最后恢复到上一次master的状态，整个恢复需要一定的时间，一般就是1-2分钟。整个spark集群master正处于恢复阶段，对任务有没有影响？（1）对于正在运行的任务有没有影响，没有影响。 由于任务正在运行，就说明它已经获取得到了资源，既然有资源了，就跟master没有关系，任务可以继续运行。这里就是没有任何影响 （2）对于正准备提交的任务有没有影响，有影响。 由于没有这样一个活着的master去分配资源，也就说任务获取不到资源，既然没有资源，任务也就无法运行，必须等到活着的master出现之后，才可以申请到资源。 7、spark集群架构 1、Driver端 它会运行客户端的main方法和构建SparkContext对象，SparkContext对象是所有spark程序的执行入口 2、Application 它就是一个spark的应用程序，包括了Driver的代码和任务运行的时候需要的资源信息 3、ClusterManager 它是给计算任务提供计算资源的外部服务 standAlone 它是spark集群自带的模式，任务的资源分配由老大Master节点负责 yarn 可以把spark程序提交到yarn中去运行，任务的资源分配由yarn中老大ResourceManager负责 mesos 它也是一个类似于yarn资源调度平台 4、Master 它是整个spark集群的老大，负责资源的分配 5、Worker 它是整个spark集群的小弟，负责任务的计算节点 6、Executor 它是一个进程，在这里表示计算的资源 它会在worker节点启动executor进程 7、task 它就是一个线程 它是以task线程的方式运行在worker节点对应的executor进程中 8、初识spark程序 1、普通模式提交任务（就是我们已经知道了集群中活着的master地址） 1234567bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node-1:7077 \--executor-memory 1G \--total-executor-cores 2 \examples/jars/spark-examples_2.11-2.1.3.jar \100 2、高可用模式下提交任务（就是集群中有很多个master，这个时候我也不知道哪一个master是活着的master） code here123456789101112bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node-1:7077,node-2:7077,node-3:7077 \--executor-memory 1G \--total-executor-cores 2 \examples/jars/spark-examples_2.11-2.1.3.jar \100后期再实际环境中，有很多个master，任务的提交需要找到活着的master去申请资源，这个时候我们不知道哪一个master是活着的master，这里我们就可以把所有的master都罗列出来：--master spark://node1:7077,node2:7077,node3:7077后期程序在运行的时候，它会通过轮训的机制最后找到活着的master，然后向活着的master申请计算资源。 9、spark-shell使用9.1 通过spark-shell –master local[N] 读取本地数据文件实现单词统计 –master local[N] local表示本地运行，跟spark集群没有任何关系 N表示一个正整数，在这里local[N] 表示本地采用N个线程去跑任务 spark-shell –master local[2] 代码 123sc.textFile("file:///root/words.txt").flatMap(x=&gt;x.split(" ")).map(x=&gt;(x,1)).reduceByKey((x,y)=&gt;x+y).collectsc.textFile("file:///root/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect 9.2 通过spark-shell –master local[N] 读取HDFS数据文件实现单词统计 spark整合HDFS vim spark-env.sh 12#spark整合HDFSexport HADOOP_CONF_DIR=/export/servers/hadoop/etc/hadoop spark-shell –master local[2] 代码 123sc.textFile("hdfs://node1:9000/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collectxsc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect 9.3 通过spark-shell 指定具体活着的master实现HDFS上文件单词统计，把结果数据保存到hdfs上 spark-shell –master spark://node2:7077 –executor-memory 1g –total-executor-cores 4 代码 1sc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("/out") 10、通过IDEA开发spark的程序10.1 利用scala语言开发spark的wordcount程序（本地运行） 1、引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.3&lt;/version&gt;&lt;/dependency&gt; 2、代码开发 12345678910111213141516171819202122232425262728293031323334353637383940package cn.itcast.sparkimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;//todo:利用scala语言开发spark的wordcount程序（本地运行）object WordCount &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 设置application名称和master地址 local[2]表示本地采用2个线程跑任务 val sparkConf: SparkConf = new SparkConf().setAppName("WordCount").setMaster("local[2]") //2、创建SparkContext对象 它是所有spark程序的执行入口，它内部会构建DAGScheduler和TaskScheduler val sc = new SparkContext(sparkConf) //设置日志输出级别 sc.setLogLevel("warn") //3、读取数据文件 val data: RDD[String] = sc.textFile("E:\\words.txt") //4、切分每一行，获取所有的单词 val words: RDD[String] = data.flatMap(x=&gt;x.split(" ")) //5、把每个单词计为1 val wordAndOne: RDD[(String, Int)] = words.map(x=&gt;(x,1)) //6、相同单词出现的1累加 val result: RDD[(String, Int)] = wordAndOne.reduceByKey((x:Int,y:Int)=&gt;x+y) //按照单词出现的次数降序排列 默认第二个参数是true表示升序，改为false为降序 val sortRDD: RDD[(String, Int)] = result.sortBy(x=&gt;x._2,false) //7、收集打印 val finalResult: Array[(String, Int)] = sortRDD.collect() finalResult.foreach(println) //8、关闭 sc.stop() &#125;&#125; 10.2 利用scala语言开发spark的wordcount程序（集群运行） 1、代码开发 1234567891011121314151617181920212223242526272829303132333435363738package cn.itcast.sparkimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDD//todo:利用scala语言开发spark的wordcount程序（集群运行）object WordCount_Online &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 设置application名称 val sparkConf: SparkConf = new SparkConf().setAppName("WordCount_Online") //2、创建SparkContext对象 它是所有spark程序的执行入口，它内部会构建DAGScheduler和TaskScheduler val sc = new SparkContext(sparkConf) //设置日志输出级别 sc.setLogLevel("warn") //3、读取数据文件 val data: RDD[String] = sc.textFile(args(0)) //4、切分每一行，获取所有的单词 val words: RDD[String] = data.flatMap(x=&gt;x.split(" ")) //5、把每个单词计为1 val wordAndOne: RDD[(String, Int)] = words.map(x=&gt;(x,1)) //6、相同单词出现的1累加 val result: RDD[(String, Int)] = wordAndOne.reduceByKey((x:Int,y:Int)=&gt;x+y) //7、把结果数据保存到hdfs上 result.saveAsTextFile(args(1)) //8、关闭 sc.stop() &#125;&#125; 2、把程序打成jar包提交到spark集群中运行 1234567spark-submit \--master spark://node-1:7077 \--class cn.itcast.spark.WordCount_Online \ #jar包的main方法路径--executor-memory 1G \--total-executor-cores 2 \original-spark_demo-1.0-SNAPSHOT.jar \ #需要在当前路径上有才能这么些，否则需要写全路径/words.txt /out_spark 10.3 利用java语言开发spark的wordcount程序（本地运行） 1、代码开发 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package cn.itcast.spark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;import java.util.List;//todo:利用java语言开发spark的wordcount程序（本地运行）public class WordCount_Java &#123; public static void main(String[] args) &#123; //1、创建SparkConf SparkConf sparkConf = new SparkConf().setAppName("WordCount_Java").setMaster("local[2]"); //2、创建JavaSparkContext JavaSparkContext jsc = new JavaSparkContext(sparkConf); //3、读取数据文件 JavaRDD&lt;String&gt; data = jsc.textFile("E:\\words.txt"); //4、切分每一行获取所有的单词 JavaRDD&lt;String&gt; words = data.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; public Iterator&lt;String&gt; call(String line) throws Exception &#123; String[] words = line.split(" "); return Arrays.asList(words).iterator(); &#125; &#125;); //5、每个单词计为1 （单词，1） JavaPairRDD&lt;String, Integer&gt; wordAndOne = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(word, 1); &#125; &#125;); //6、相同单词出现的1累加 JavaPairRDD&lt;String, Integer&gt; result = wordAndOne.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); //按照单词出现的次数降序排列 (单词，次数)-------&gt;(次数，单词).sortByKey-------&gt;(单词，次数) JavaPairRDD&lt;Integer, String&gt; reverseRDD = result.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123; public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123; return new Tuple2&lt;Integer, String&gt;(t._2, t._1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; sortedRDD = reverseRDD.sortByKey(false).mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123; public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; t) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(t._2, t._1); &#125; &#125;); //7、收集打印 List&lt;Tuple2&lt;String, Integer&gt;&gt; finalResult = sortedRDD.collect(); for (Tuple2&lt;String, Integer&gt; tuple : finalResult) &#123; System.out.println("单词："+tuple._1+" 次数："+tuple._2); &#125; //8、关闭 jsc.stop(); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper集群搭建]]></title>
    <url>%2Fblog%2F2019%2F03%2F01%2FZookeepr%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"></content>
      <categories>
        <category>大数据</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>搭建</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
</search>
