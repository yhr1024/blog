<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SparkStreaming&Kafka&Flume整合]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2FSparkStreaming%26Kafka%26Flume%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[整体流程 12345graph LR Flume集群--&gt;|Kafka集群Poll数据|Kafka集群((Kafka集群)) Kafka集群---|Spark集群Poll数据|Spark集群 1.Flume与Kafka整合1.安装部署好Flume与Kafka集群 2.配置flume-kafka.conf 12345678910111213141516171819202122#为我们的source channel sink起名a1.sources = r1a1.channels = c1a1.sinks = k1#指定我们的source收集到的数据发送到哪个管道a1.sources.r1.channels = c1#指定我们的source数据收集策略a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /export/servers/flumedataa1.sources.r1.deletePolicy = nevera1.sources.r1.fileSuffix = .COMPLETEDa1.sources.r1.ignorePattern = ^(.)*\\.tmp$a1.sources.r1.inputCharset = utf-8#指定我们的channel为memory,即表示所有的数据都装进memory当中a1.channels.c1.type = memory#指定我们的sink为kafka sink，并指定我们的sink从哪个channel当中读取数据a1.sinks.k1.channel = c1a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic = testa1.sinks.k1.kafka.bootstrap.servers = node-1:9092,node-2:9092,node-3:9092a1.sinks.k1.kafka.flumeBatchSize = 20a1.sinks.k1.kafka.producer.acks = 1 3.启动Kafka集群 4.在Flume安装目录下启动Flume 1bin/flume-ng agent -n a1 -c conf -f conf/flume-kafka.conf -Dflume.root.logger=INFO,console 2.Kafka与Spark整合]]></content>
      <categories>
        <category>大数据</category>
        <category>整合</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>spark</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka的使用]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2FKafka%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.kafka的命令行的管理使用现在有三台服务器组成的Kafka集群（node-1,node-2,node-3） 官网操作手册：https://kafka.apache.org/quickstart 1.创建topic1234567891011121314kafka-topics.sh \--create \--topic KafkaTest \--partitions 3 \--replication-factor 2 \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--create:表示创建--topic：指定要创建的topic名称--partitions： 指定topic的分区数--replication-factor：指定每一个分区的副本数--zookeeper ：指定依赖的zk集群地址 2.显示所有的topic1234567kafka-topics.sh \--list \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--list:查看所有的topic信息--zookeeper ：指定依赖的zk集群地址 3.模拟生产者1234567kafka-console-producer.sh \--broker-list node-1:9092,node-2:9092,node-3:9092 \--topic KafkaTest#参数说明--topic：指定topic的名称--broker-list ：指定kafka集群地址 4.模拟消费者123456789kafka-console-consumer.sh \--from-beginning \--topic KafkaTest \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--bootstrap-server:指定kafka集群地址--topic:指定topic的名称--from-beginning:从最开始的第一条数据开始消费 5.删除topic123456789kafka-topics.sh \--delete \--topic test \--zookeeper node-1:2181,node-2:2181,node-3:2181#参数说明--delete ：表示要删除topic--topic：指定topic的名称--zookeeper：指定zookeeper的地址 注意事项 如果缺少所需要的参数，会有错误提示。根据错误提示添加所需要的参数 12#例如少写了 --zookeeper参数，会提示Missing required argument "[zookeeper]" 2.Kafka的API1.导入所依赖的jar包12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 2.Kafka生产者代码1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;//TODO：开发一个Kafka生产者代码public class KafkaProducerDemo1 &#123; public static void main(String[] args) &#123; Properties props = new Properties(); //todo:指定kafka集群地址 props.put("bootstrap.servers", "node-1:9092,node-2:9092,node-3:9092"); //todo:消息的确认机制 props.put("acks", "all"); //todo:重试的次数 props.put("retries", 0); //todo:设置批量的导入数据的大小 props.put("batch.size", 16384); //todo:表示延迟时间 props.put("linger.ms", 1); //todo:缓冲区的内存大小 props.put("buffer.memory", 33554432); //todo:序列化key值 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //todo:序列化value值 props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); //todo:增加ack机制 props.put("acks", "all"); //todo:添加自定义的分区规则 props.put("partitioner.class", "MyPartitioner"); //new KafkaProducer&lt;String, String&gt;(props) //todo:7.有2个String类型的泛型，第一个String表示消息的key的类型，key是消息的标识，第二个String表示消息的内容 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 0; i &lt; 100; i++) //第一个参数表示topic名称，第二个参数表示消息的key，第三个参数表示消息的内容 producer.send(new ProducerRecord&lt;String, String&gt;("test", Integer.toString(i),"hadoop spark")); producer.close(); &#125;&#125; 3.Kafka消费者代码自动提交偏移量 12345678910111213141516171819202122232425262728293031323334import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;//todo:开发一个kafka消费者代码public class KafkaConsumerDemo1 &#123; public static void main(String[] args) &#123; Properties props = new Properties(); //todo:指定kafka集群地址 props.put("bootstrap.servers", "node-1:9092,node-2:9092,node-3:9092"); //todo:消费者组id props.put("group.id", "test"); //todo:消息的偏移量是自动提交 props.put("enable.auto.commit", "true"); //todo:消息自动提交偏移量的时间间隔 props.put("auto.commit.interval.ms", "1000"); //todo:key的反序列化 props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); //todo:value的反序列化 props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //todo:指定要消费的topic名称 consumer.subscribe(Arrays.asList("test")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) //System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); System.out.println("offset:"+record.offset()+" key:"+record.key()+" value："+record.value()+" partition:"+record.partition()); &#125; &#125;&#125; 4.Kafka的自定义分区策略4.1直接指定分区12//第一个参数表示topic名称，第二个参数表示分区号，第三个参数表示消息的key，第四个参数表示消息的内容kafkaProducer.send(new ProducerRecord&lt;String, String&gt;("testpart",1,"0","value"+i)); 4.2自定义分区123456789101112131415161718192021222324252627282930313233import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import java.util.Map;//自定义分区public class MyPartitioner implements Partitioner&#123; /** * * @param topic topic名称 * @param key 消息的key * @param keyBytes 消息的key的字节数组 * @param value 消息的内容 * @param valueBytes 消息的内容字节数组 * @param cluster kafka集群对象 * @return */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; // key.hashcode % 分区数=分区号-----------hashPartitioner int partitions = cluster.partitionsForTopic(topic).size(); //(-2,-1,0,1,2) int partition = Math.abs(key.hashCode() % partitions); return partition; &#125; public void close() &#123; &#125; public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; 5.Kafka12]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka集群搭建]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2FKafka%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Kafka集群搭建假设有node-1,node-2,node-3三台服务器 1.搭建一个ZK集群 2.下载对应的安装包：https://kafka.apache.org/downloads 3.上传到node-1服务器，并解压到指定目录 在这里指定安装目录为：”/export/servers” 可以重命名目录，在这里改为 “kafka” 4.修改配置文件 进入到kafka安装目录下的config目录 vim server.properties 1234567891011121314#指定每台节点的brokerid 必须唯一，不能够重复broker.id=0#指定这个broker的主机名host.name=node-1#指定topic的数据是否可以删除，默认等于false表示不可以，改为true表示可以delete.topic.enable=true#指定topic的数据存放目录log.dirs=/export/servers/kafka/kafka-logs#指定依赖的zk集群地址zookeeper.connect=node-1:2181,node-2:2181,node-3:2181 5.配置kafka环境变量 vim /etc/profile 12export KAFKA_HOME=/export/servers/kafkaexport PATH=$PATH:$KAFKA_HOME/bin 6.分发到其他服务器上 12345scp -r kafka node-2:/export/serversscp -r kafka node-3:/export/serversscp /etc/profile node-2:/etcscp /etc/profile node-3:/etc 7.修改node-2和node-3上的配置 node-2需要修改的部分 12345#指定每台节点的brokerid 必须唯一，不能够重复broker.id=1#指定这个broker的主机名host.name=node-2 node-3需要修改的部分 12345#指定每台节点的brokerid 必须唯一，不能够重复broker.id=2#指定这个broker的主机名host.name=node-3 8.让所有kafka节点的环境变量生效 在所有的节点执行1source /etc/profile Kafka集群启动&amp;关闭1.启动集群1.可以手动启动每一个节点 12#在bin目录下nohup kafka-server-start.sh config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp; 2.可以编写一键启动脚本 1234567891011#!/bin/bash#指定Kafka集群的IP,并利用for循环依次启动for host in &#123;node-1,node-2,node-3&#125;do #1.重载环境变量 #2.调用bin目录下的kafka-server-start.sh 启动Kafka #3.丢弃日志信息 ssh $host "source /etc/profile;/export/servers/kafka/bin/kafka-server-start.sh \ /export/servers/kafka/config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;" echo "$host kafaka is running"done 2.关闭集群1.可以手动关闭每一个节点 12#在bin目录下kafka-server-stop.sh 这个脚本有一定的问题（centos6.x有问题，如果是centos 7.x是没有问题）可以用一下方法修改 1234#vi kafka-server-stop.sh 将PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')#改为PIDS=$(ps ax | grep -i 'kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;') 2.可以编写shell脚本一键关闭 123456#!/bin/bashfor host in &#123;node-1,node-2,node-3&#125;do ssh $host "/export/servers/kafka/bin/kafka-server-stop.sh stop" echo "$host kafaka is stop"done]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
        <category>集群搭建</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka集群搭建</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka简介]]></title>
    <url>%2Fblog%2F2019%2F03%2F09%2FKafka%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1.Kafka概述Kafka官网：https://kafka.apache.org/ Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。 Kafka最初是由LinkedIn开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高吞吐量、低延迟的平台。 Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。 Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性 2.Kafka集群架构 3.kafka组件介绍Topic ：消息根据Topic进行归类 Producer：发送消息者 Consumer：消息接受者 broker：每个kafka实例(server) Zookeeper：依赖集群保存meta信息。 Topics组件Topic：一类消息，每个topic将被分成多个partition(区)，在集群的配置文件中配置。 partition：在存储层面是逻辑append log文件，包含多个segment文件。 Segement：消息存储的真实文件，会不断生成新的。 offset：每条消息在文件中的位置（偏移量）。offset为一个long型数字，它是唯一标记一条消息。 Partition1.在存储层面是逻辑append log文件，每个partition有多个segment组成。 2.任何发布到此partition的消息都会被直接追加到log文件的尾部。 3.每个partition在内存中对应一个index列表，记录每个segment中的第一条消息偏移。这样查找消息的时候，先在index列表中定位消息位置，再读取文件，速度块。 4.发布者发到某个topic的消息会被均匀的分布到多个part上，broker收到发布消息往对应part的最后一个segment上添加该消息。 3.Kafka存储机制存储机制概述 Kafka中的Message是以topic为基本单位组织的，不同的topic之间是相互独立的。每个topic又可以分成几个不同的partition，每个partition存储一部分Message。 Partition是以文件的形式存储在文件系统中比如，创建了一个名为”test”的topic，其有3个partition，那么在Kafka的数据目录中(由配置文件中的log.dirs指定的)中就会有这样5个目录: test-0，test-1，test-2其命名规则为&lt;topic_name&gt;-&lt;partition_id&gt;，里面存储的分别就是这3个partition的数据。 每一个partition目录下的文件被平均切割成大小相等（默认一个文件是1G，可以手动去设置）的数据文件，每一个数据文件都被称为一个段（segment file），但每个段消息数量不一定相等，这种特性能够使得老的segment可以被快速清除。默认保留7天的数据。 每次满1G后，在写入到一个新的文件中。 另外每个partition只需要支持顺序读写就可以。如上图所示： 首先00000000000000000000.log是最早产生的文件，该文件达到1G后又产生了新的00000000000009084544.log文件，新的数据会写入到这个新的文件里面。这个文件到达1G后，数据又会写入到下一个文件中。也就是说它只会往文件的末尾追加数据，这就是顺序写的过程，生产者只会对每一个partition做数据的追加（写操作）。 12· 数据消费问题讨论问题：如何保证消息消费的有序性呢？比如说生产者生产了0到100个商品，那么消费者在消费的时候按照0到100这个从小到大的顺序消费， 那么kafka如何保证这种有序性呢？ 难度就在于，生产者生产出0到100这100条数据之后，通过一定的分组策略存储到broker的partition中的时候， 比如0到10这10条消息被存到了这个partition中，10到20这10条消息被存到了那个partition中，这样的话，消息在分组存到partition中的时候就已经被分组策略搞得无序了。 那么能否做到消费者在消费消息的时候全局有序呢？ 遇到这个问题，我们可以回答，在大多数情况下是做不到全局有序的。但在某些情况下是可以做到的。比如我的partition只有一个，这种情况下是可以全局有序的。 那么可能有人又要问了，只有一个partition的话，哪里来的分布式呢？哪里来的负载均衡呢？ 所以说，全局有序是一个伪命题！全局有序根本没有办法在kafka要实现的大数据的场景来做到。但是我们只能保证当前这个partition内部消息消费的有序性。 结论：一个partition中的数据是有序的吗？ 回答：间隔有序，不连续。 Segment文件Segment file是什么？ ​ 生产者生产的消息按照一定的分组策略被发送到broker中partition中的时候，这些消息如果在内存中放不下了，就会放在文件中。 ​ partition在磁盘上就是一个目录，该目录名是topic的名称加上一个序号，在这个partition目录下，有两类文件，一类是以log为后缀的文件，一类是以index为后缀的文件，每一个log文件和一个index文件相对应，这一对文件就是一个segment file，也就是一个段。 ​ 其中的log文件就是数据文件，里面存放的就是消息，而index文件是索引文件，索引文件记录了元数据信息 Segment文件特点 ​ segment文件命名的规则：partition全局的第一个segment从0（20个0）开始，后续的每一个segment文件名是上一个segment文件中最后一条消息的offset值。 ​ 那么这样命令有什么好处呢？假如我们有一个消费者已经消费到了368776（offset值为368776），那么现在我们要继续消费的话，怎么做呢？ ​ 看上图，分2个步骤，第1步是从所有文件log文件的的文件名中找到对应的log文件，第9100000条数据位于上图中的“00000000000009084544.log”这个文件中， ​ 这一步涉及到一个常用的算法叫做“二分查找法”（假如我现在给你一个offset值让你去找，你首先是将所有的log的文件名进行排序，然后通过二分查找法进行查找， ​ 很快就能定位到某一个文件，紧接着拿着这个offset值到其索引文件中找这条数据究竟存在哪里）；第2步是到index文件中去找第9100000条数据所在的位置。 ​ 索引文件（index文件）中存储这大量的元数据，而数据文件（log文件）中存储这大量的消息。 ​ 索引文件（index文件）中的元数据指向对应的数据文件（log文件）中消息的物理偏移地址。 kafka如何快速查询数据 上图的左半部分是索引文件，里面存储的是一对一对的key-value，其中key是消息在数据文件（对应的log文件）中的编号，比如“1,3,6,8……”， 分别表示在log文件中的第1条消息、第3条消息、第6条消息、第8条消息……，那么为什么在index文件中这些编号不是连续的呢？ 这是因为index文件中并没有为数据文件中的每条消息都建立索引，而是采用了稀疏存储的方式，每隔一定字节的数据建立一条索引。 这样避免了索引文件占用过多的空间，从而可以将索引文件保留在内存中。 但缺点是没有建立索引的Message也不能一次定位到其在数据文件的位置，从而需要做一次顺序扫描，但是这次顺序扫描的范围就很小了。 其中以索引文件中元数据3,497为例，其中3代表在右边log数据文件中从上到下第3个消息(在全局partiton表示第368772个消息)， 其中497表示该消息的物理偏移地址（位置）为497。 存储机制总结Kafka高效文件存储设计特点：• Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。• 通过索引信息可以快速定位message。• 通过index元数据全部映射到memory，可以避免segmentfile的IO磁盘操作。• 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 4.Kafka的如何保证 数据不丢失4.1生产者如何保证数据的不丢失 Kafka的ack机制：在kafka发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到。 如果是同步模式：ack机制能够保证数据的不丢失 在config目录中的producer.properties设置 123#同步模式producer.type=sync request.required.acks=1 其中 acks=0 表示不等待broker的确认信息，最小延迟acks=1 表示partition leader接收到确认信息后，broker就返回成功。不管其他的副本是否成功（Replica）acks=all 或者 acks=-1 表示只有producer全部写成功的时候，才算成功 如果是异步模式：通过buffer来进行控制数据的发送，有两个值来进行控制，时间阈值与消息的数量阈值，如果buffer满了数据还没有发送出去，如果设置的是立即清理模式，风险很大，一定要设置为阻塞模式。 结论：producer有丢数据的可能，但是可以通过配置保证消息的不丢失 123456producer.type=async request.required.acks=1 queue.buffering.max.ms=5000 queue.buffering.max.messages=10000 queue.enqueue.timeout.ms = -1 batch.num.messages=200 4.2 消费者如何保证数据的不丢失通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的offset数值，下次继续消费的时候，接着上次的offset进行消费即可。]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark基础]]></title>
    <url>%2Fblog%2F2019%2F03%2F09%2Fspark%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[spark基础1、目标 1、掌握spark相关概念 2、掌握搭建一个spark集群 3、掌握开发简单的spark应用程序 2、spark概述 2.1 spark是什么 Apache Spark™ is a unified analytics engine for large-scale data processing. apache的spark是一个针对于大规模数据处理的统一分析引擎 spark是基于内存计算的分布式分析引擎，计算速度非常快，这里仅仅只涉及到数据的计算，并没有涉及到数据的存储，后期再使用spark的时候，就需要对接一些外部的数据源（比如HDFS） 2.2 为什么要学习spark1由于spark的处理速度比mapreduce快很多，很受企业青睐，所以我今天就重点学习它 2.3 spark框架的四大特性 1、速度快 spark比mapreduce处理任务在内存中快100倍，在磁盘中快10倍 spark比mapreduce速度快的2个主要原因 12345（1） mapreduce任务每一次job的输出结果只能够保存在磁盘中，后续有其他的job需要依赖于前面job的输出结果，这个时候只能够进行大量的磁盘io操作，获取得到。spark任务每一次的job输出结果可以直接保存在内存中，后续有其他的job需要依赖于前面job的输出结果，这个时候就可以直接从内存中获取得到。大大减少磁盘io操作，最后整体上提升性能。 hivesql: select name,age,sex from (select * from user where age &gt;30) (2) mapreduce任务它是以进程的方式运行在yarn中，比如一个mapreduce任务中，有100个MapTask,这个时候就需要开启100个进程去运行着100个task；spark任务的它是以线程的方式运行在进程中，比如一个spark任务中还是有100个task，后期再运行的时候，可以极端一点，开启一个进程，在这一个进程中运行100个线程即可。 开启一个进程跟开启一个线程的代价肯定是不一样，开启一个进程需要的时间远远高于开启一个线程需要的时间。 2、易用性 可以快速开发一个spark应用程序，通过java、scala、python、R、SQL等不同的语言进行代码开发 3、通用性 spark框架不在是一个简单的框架，它发展成一个spark的生态系统，它是包括了很多不同的子项目 Sparksql sparkStreaming Mlib Graphx 4、兼容性 spark程序就是一个计算任务，哪里可以给当前这个任务提供计算资源，我们就可以把spark程序提交到哪里去运行 standAlone 它是spark集群自带的模式，整个任务的资源分配由spark集群中的老大Master节点负责 yarn 可以把spark程序提交到yarn中去运行，整个任务的资源分配由yarn中的老大ResouceManager mesos 它也是一个类似于yarn的开源的资源调度框架 3、spark集群安装部署 1、下载对应的安装包 http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.1.3/spark-2.1.3-bin-hadoop2.7.tgz spark-2.1.3-bin-hadoop2.7.tgz 2、规划安装目录 /export/servers 3、上传安装包到服务器中 4、解压安装包到指定的规划目录 tar -zxvf spark-2.1.3-bin-hadoop2.7.tgz -C /export/servers 5、重命名解压目录 mv spark-2.1.3-bin-hadoop2.7 spark 6、修改配置文件 进入到spark的安装目录有一个conf文件夹 vim spark-env.sh ( mv spark-env.sh.template spark-env.sh) 123456#配置java环境变量export JAVA_HOME=/export/servers/jdk#指定master的地址export SPARK_MASTER_HOST=node1#指定master的端口export SPARK_MASTER_PORT=7077 vim slaves (mv slaves.template slaves) 123#指定哪些节点是workernode2node3 7、配置spark环境变量 vim /etc/profile 12export SPARK_HOME=/export/servers/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 8、分发spark目录和环境变量 12345scp -r spark node2:/export/serversscp -r spark node3:/export/serversscp /etc/profile node2:/etcscp /etc/profile node3:/etc 9、让所有spark节点的环境变量生效 可以在所有节点执行 source /etc/profile 4、spark集群的启动和停止 1、启动spark集群 在主节点执行脚本 sbin/start-all.sh 首先在主节点启动了一个Master进程，它是整个spark集群的老大，负责任务的资源分配 通过slaves文件执行的worker节点来分别启动worker进程 2、停止spark集群 在主节点执行脚本 sbin/stop-all.sh 5、spark集群的web管理界面 1、启动好spark集群之后 可以访问地址 master主机名:8080 可以看到spark集群的所有信息 spark集群的总的资源信息 spark集群已经使用的资源信息 spark集群还剩的资源信息 spark集群中每一个worker的相关信息 spark集群中正在运行的任务信息 spark集群中已经完成的任务信息 6、基于zk构建高可用的spark集群 1、事先搭建一个zk集群 2、修改配置 vim spark-env.sh 12345#把之前手动指定哪一个节点是活着的master这个参数注释掉#export SPARK_MASTER_HOST=node1#引入zk，构建高可用的spark集群export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 3、分发文件到其他机器 12scp spark-env.sh node2:/export/servers/spark/confscp spark-env.sh node3:/export/servers/spark/conf 4、先启动zk 5、启动spark集群 1、可以在任意一台机器来启动一个脚本（前提条件：实现任意2台机器之间的ssh免密登录） sbin/start-all.sh 2、它会在当前机器启动一个Master进程（活着的master） 3、整个spark集群中的worker进程由slaves文件决定 4、在其他机器单独启动master sbin/start-master.sh master恢复逻辑 12345678910首先基于引入了zk集群，这个时候整个spark集群中有很多个master，其中一个master被选举成活着的master，它提供服务，后期给任务分配资源。还有其他多个master被选举成备用的master（standBy）,它不提供服务。当前活着的master挂掉了，首先zk会感知到，接下来在所有处于standBy中的master进行选举，最后生成一个活着的master，后期需要从zk集群中读取保存了spark集群的元数据节点spark。最后恢复到上一次master的状态，整个恢复需要一定的时间，一般就是1-2分钟。整个spark集群master正处于恢复阶段，对任务有没有影响？（1）对于正在运行的任务有没有影响，没有影响。 由于任务正在运行，就说明它已经获取得到了资源，既然有资源了，就跟master没有关系，任务可以继续运行。这里就是没有任何影响 （2）对于正准备提交的任务有没有影响，有影响。 由于没有这样一个活着的master去分配资源，也就说任务获取不到资源，既然没有资源，任务也就无法运行，必须等到活着的master出现之后，才可以申请到资源。 7、spark集群架构 1、Driver端 它会运行客户端的main方法和构建SparkContext对象，SparkContext对象是所有spark程序的执行入口 2、Application 它就是一个spark的应用程序，包括了Driver的代码和任务运行的时候需要的资源信息 3、ClusterManager 它是给计算任务提供计算资源的外部服务 standAlone 它是spark集群自带的模式，任务的资源分配由老大Master节点负责 yarn 可以把spark程序提交到yarn中去运行，任务的资源分配由yarn中老大ResourceManager负责 mesos 它也是一个类似于yarn资源调度平台 4、Master 它是整个spark集群的老大，负责资源的分配 5、Worker 它是整个spark集群的小弟，负责任务的计算节点 6、Executor 它是一个进程，在这里表示计算的资源 它会在worker节点启动executor进程 7、task 它就是一个线程 它是以task线程的方式运行在worker节点对应的executor进程中 8、初识spark程序 1、普通模式提交任务（就是我们已经知道了集群中活着的master地址） 1234567bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node-1:7077 \--executor-memory 1G \--total-executor-cores 2 \examples/jars/spark-examples_2.11-2.1.3.jar \100 2、高可用模式下提交任务（就是集群中有很多个master，这个时候我也不知道哪一个master是活着的master） code here123456789101112bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node-1:7077,node-2:7077,node-3:7077 \--executor-memory 1G \--total-executor-cores 2 \examples/jars/spark-examples_2.11-2.1.3.jar \100后期再实际环境中，有很多个master，任务的提交需要找到活着的master去申请资源，这个时候我们不知道哪一个master是活着的master，这里我们就可以把所有的master都罗列出来：--master spark://node1:7077,node2:7077,node3:7077后期程序在运行的时候，它会通过轮训的机制最后找到活着的master，然后向活着的master申请计算资源。 9、spark-shell使用9.1 通过spark-shell –master local[N] 读取本地数据文件实现单词统计 –master local[N] local表示本地运行，跟spark集群没有任何关系 N表示一个正整数，在这里local[N] 表示本地采用N个线程去跑任务 spark-shell –master local[2] 代码 123sc.textFile("file:///root/words.txt").flatMap(x=&gt;x.split(" ")).map(x=&gt;(x,1)).reduceByKey((x,y)=&gt;x+y).collectsc.textFile("file:///root/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect 9.2 通过spark-shell –master local[N] 读取HDFS数据文件实现单词统计 spark整合HDFS vim spark-env.sh 12#spark整合HDFSexport HADOOP_CONF_DIR=/export/servers/hadoop/etc/hadoop spark-shell –master local[2] 代码 123sc.textFile("hdfs://node1:9000/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collectxsc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect 9.3 通过spark-shell 指定具体活着的master实现HDFS上文件单词统计，把结果数据保存到hdfs上 spark-shell –master spark://node2:7077 –executor-memory 1g –total-executor-cores 4 代码 1sc.textFile("/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("/out") 10、通过IDEA开发spark的程序10.1 利用scala语言开发spark的wordcount程序（本地运行） 1、引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.3&lt;/version&gt;&lt;/dependency&gt; 2、代码开发 12345678910111213141516171819202122232425262728293031323334353637383940package cn.itcast.sparkimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;//todo:利用scala语言开发spark的wordcount程序（本地运行）object WordCount &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 设置application名称和master地址 local[2]表示本地采用2个线程跑任务 val sparkConf: SparkConf = new SparkConf().setAppName("WordCount").setMaster("local[2]") //2、创建SparkContext对象 它是所有spark程序的执行入口，它内部会构建DAGScheduler和TaskScheduler val sc = new SparkContext(sparkConf) //设置日志输出级别 sc.setLogLevel("warn") //3、读取数据文件 val data: RDD[String] = sc.textFile("E:\\words.txt") //4、切分每一行，获取所有的单词 val words: RDD[String] = data.flatMap(x=&gt;x.split(" ")) //5、把每个单词计为1 val wordAndOne: RDD[(String, Int)] = words.map(x=&gt;(x,1)) //6、相同单词出现的1累加 val result: RDD[(String, Int)] = wordAndOne.reduceByKey((x:Int,y:Int)=&gt;x+y) //按照单词出现的次数降序排列 默认第二个参数是true表示升序，改为false为降序 val sortRDD: RDD[(String, Int)] = result.sortBy(x=&gt;x._2,false) //7、收集打印 val finalResult: Array[(String, Int)] = sortRDD.collect() finalResult.foreach(println) //8、关闭 sc.stop() &#125;&#125; 10.2 利用scala语言开发spark的wordcount程序（集群运行） 1、代码开发 1234567891011121314151617181920212223242526272829303132333435363738package cn.itcast.sparkimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDD//todo:利用scala语言开发spark的wordcount程序（集群运行）object WordCount_Online &#123; def main(args: Array[String]): Unit = &#123; //1、创建SparkConf对象 设置application名称 val sparkConf: SparkConf = new SparkConf().setAppName("WordCount_Online") //2、创建SparkContext对象 它是所有spark程序的执行入口，它内部会构建DAGScheduler和TaskScheduler val sc = new SparkContext(sparkConf) //设置日志输出级别 sc.setLogLevel("warn") //3、读取数据文件 val data: RDD[String] = sc.textFile(args(0)) //4、切分每一行，获取所有的单词 val words: RDD[String] = data.flatMap(x=&gt;x.split(" ")) //5、把每个单词计为1 val wordAndOne: RDD[(String, Int)] = words.map(x=&gt;(x,1)) //6、相同单词出现的1累加 val result: RDD[(String, Int)] = wordAndOne.reduceByKey((x:Int,y:Int)=&gt;x+y) //7、把结果数据保存到hdfs上 result.saveAsTextFile(args(1)) //8、关闭 sc.stop() &#125;&#125; 2、把程序打成jar包提交到spark集群中运行 1234567spark-submit \--master spark://node-1:7077 \--class cn.itcast.spark.WordCount_Online \ #jar包的main方法路径--executor-memory 1G \--total-executor-cores 2 \original-spark_demo-1.0-SNAPSHOT.jar \ #需要在当前路径上有才能这么些，否则需要写全路径/words.txt /out_spark 10.3 利用java语言开发spark的wordcount程序（本地运行） 1、代码开发 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package cn.itcast.spark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;import java.util.List;//todo:利用java语言开发spark的wordcount程序（本地运行）public class WordCount_Java &#123; public static void main(String[] args) &#123; //1、创建SparkConf SparkConf sparkConf = new SparkConf().setAppName("WordCount_Java").setMaster("local[2]"); //2、创建JavaSparkContext JavaSparkContext jsc = new JavaSparkContext(sparkConf); //3、读取数据文件 JavaRDD&lt;String&gt; data = jsc.textFile("E:\\words.txt"); //4、切分每一行获取所有的单词 JavaRDD&lt;String&gt; words = data.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; public Iterator&lt;String&gt; call(String line) throws Exception &#123; String[] words = line.split(" "); return Arrays.asList(words).iterator(); &#125; &#125;); //5、每个单词计为1 （单词，1） JavaPairRDD&lt;String, Integer&gt; wordAndOne = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(word, 1); &#125; &#125;); //6、相同单词出现的1累加 JavaPairRDD&lt;String, Integer&gt; result = wordAndOne.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); //按照单词出现的次数降序排列 (单词，次数)-------&gt;(次数，单词).sortByKey-------&gt;(单词，次数) JavaPairRDD&lt;Integer, String&gt; reverseRDD = result.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123; public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; t) throws Exception &#123; return new Tuple2&lt;Integer, String&gt;(t._2, t._1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; sortedRDD = reverseRDD.sortByKey(false).mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123; public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; t) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(t._2, t._1); &#125; &#125;); //7、收集打印 List&lt;Tuple2&lt;String, Integer&gt;&gt; finalResult = sortedRDD.collect(); for (Tuple2&lt;String, Integer&gt; tuple : finalResult) &#123; System.out.println("单词："+tuple._1+" 次数："+tuple._2); &#125; //8、关闭 jsc.stop(); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>spark</tag>
      </tags>
  </entry>
</search>
